import streamlit as st
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))
from utils.chatbot_ui import load_css

st.set_page_config(page_title="EDA", page_icon="üìà", layout="wide")
load_css("global.css", "eda.css")

BASE_DIR = Path(__file__).parent.parent  # points to app/
IMG_DIR = BASE_DIR / "assets" / "images" / "eda"
# ==================== HEADER ====================
st.title("Exploratory Data Analysis")
st.markdown("---")

st.info("""
**Analysis Overview**: This EDA examines the characteristics, distribution, and variability of four emotional 
speech datasets (RAVDESS, TESS, CREMA-D, SAVEE), focusing on class distribution, speaker diversity, acoustic 
properties, and cross-dataset variability that influence model performance.
""")

# ==================== MAIN TABS ====================
tab1, tab2, tab3, tab4 = st.tabs([
    "Dataset Overview",
    "Duration Analysis",
    "Spectral Characteristics",
    "Frequency Analysis"
])

# ==================== TAB 1: DATASET OVERVIEW ====================
with tab1:
    st.header("Dataset Composition and Class Distribution")
    
    st.write("""
    The four datasets vary substantially in size and emotional category coverage:
    - **RAVDESS** and **CREMA-D** offer the largest number of samples
    - **SAVEE** is limited to four speakers
    - **TESS** contains recordings from only two actors
    - Common emotions (happiness, sadness, neutral) appear across all datasets
    - Others (fear, disgust) are inconsistently represented
    """)
    
    st.markdown("---")
    
    # Dataset composition metrics
    st.subheader("Dataset Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total Samples", "12,162")
    col2.metric("Emotion Classes", "8")
    col3.metric("Max Duration", "7.14s")
    col4.metric("95th Percentile", "3.84s")
    
    st.markdown("---")
    
    # Emotion contribution image
    st.image(IMG_DIR / "emotion_contribution_by_dataset.png", 
            caption="Figure 1: The emotion contribution of each dataset",
            use_container_width=True)

# ==================== TAB 2: DURATION ANALYSIS ====================
with tab2:
    st.header("‚è±Audio Duration Analysis")
    
    st.write("""
    The datasets differ in duration:
    
    - **RAVDESS**: ~3 sec clips
    - **CREMA-D**: 2‚Äì4 sec
    - **TESS**: single-word recordings (~1 sec)
    - **SAVEE**: variable lengths across sentences
    
    **Longest duration**: 7.14 seconds  
    **95th percentile**: approximately 3.84 seconds
    """)
    
    st.markdown("---")
    
    # Duration statistics
    st.subheader("Duration Statistics")
    
    stat_col1, stat_col2, stat_col3, stat_col4 = st.columns(4)
    
    stat_col1.metric("Minimum", "~1.0s", "TESS single words")
    stat_col2.metric("Maximum", "7.14s", "Longest clip")
    stat_col3.metric("95th Percentile", "3.84s", "Upper typical duration")
    stat_col4.metric("Median", "~2.5s", "Typical sample length")
    
    st.markdown("---")
    
    # Before trimming
    st.subheader("Duration Distribution (Before Trimming)")
    
    st.write("""
    Raw audio files contain silence before and after speech, inflating durations:
    """)
    
    # IMAGE PLACEHOLDER 2: Figure 3 - Duration distribution before trimming
    st.image(IMG_DIR / "duration_distribution_before_trim.png", 
            caption="Figure 3: Distribution of audio duration (before trimming silence)",
            use_container_width=True)
    
    with st.expander("Analysis of Untrimmed Durations"):
        st.write("""
        ### Observations Before Trimming:
        
        - **TESS**: Shortest durations (~1-2s) due to single-word format
        - **RAVDESS**: Consistent around 3s (sentence-level recordings)
        - **CREMA-D**: Widest spread (2-7s) reflecting diverse sentence structures
        - **SAVEE**: Variable lengths with some outliers up to 7.14s
        
        **Issue**: Leading and trailing silence artificially extends duration and 
        introduces noise into time-based features.
        
        **Solution**: Apply silence trimming during preprocessing.
        """)
    
    st.markdown("---")
    
    # After trimming
    st.subheader("Duration Distribution (After Trimming)")
    
    st.write("""
    The speech records have silence before and after. After trimming, durations become more consistent:
    """)
    
    # IMAGE PLACEHOLDER 3: Figure 6 - Duration distribution after trimming
    st.image(IMG_DIR / "duration_distribution_after_trim.png", 
            caption="Figure 6: Distribution of audio duration after trimming silence",
            use_container_width=True)
    
    with st.expander("Post-Trimming Improvements"):
        st.write("""
        ### Impact of Silence Trimming:
        
        **Before Trimming:**
        - Mean duration: ~3.2s
        - Standard deviation: ~1.8s
        - Range: 1.0s - 7.14s
        
        **After Trimming:**
        - Mean duration: ~2.8s
        - Standard deviation: ~1.2s
        - Range: 0.8s - 5.2s
        
        **Benefits:**
        - More consistent duration distribution
        - Reduced variance across datasets
        - Cleaner spectral features without silence artifacts
        - Better alignment for fixed-length model inputs
        
        **Trimming Method:**
        - Threshold-based energy detection
        - Remove segments below -40 dB
        - Preserve 50ms padding at edges
        """)

# ==================== TAB 3: SPECTRAL CHARACTERISTICS ====================
with tab3:
    st.header("Spectral and Temporal Characteristics")
    
    st.write("""
    Preliminary inspection of waveforms and Mel-spectrograms revealed:
    
    - **High-arousal emotions** (anger, fear): Higher energy and broader spectral spread
    - **Low-arousal emotions** (sadness): Lower intensity and narrower spectral pattern
    
    These observations confirm the suitability of spectrogram-based inputs for deep learning models.
    """)
    
    st.markdown("---")
    
    # Waveform and spectrogram
    st.subheader("Waveform and Spectrogram Examples")
    
    st.image(IMG_DIR / "waveform_spectrogram_emotions.png", 
            use_container_width=True)
    st.image(IMG_DIR / "waveform_spectrogram_emotions_2.png", 
            use_container_width=True)
    st.image(IMG_DIR / "waveform_spectrogram_emotions_3.png", 
            caption="Figure 4: Waveform and spectrogram of emotions",
            use_container_width=True)
    
    with st.expander("Detailed Waveform Analysis"):
        st.write("""
        ### Emotion-Specific Waveform Characteristics:
        
        **Happy**:
        - Higher amplitude variations
        - Frequent peaks indicating excitement
        - Shorter pauses between speech segments
        - Average RMS energy: ~0.45
        
        **Sad**:
        - Lower overall amplitude
        - Slower variations and longer pauses
        - More uniform, subdued patterns
        - Average RMS energy: ~0.28
        
        **Angry**:
        - Highest amplitude peaks
        - Sharp, abrupt transitions
        - High energy throughout
        - Average RMS energy: ~0.62
        
        **Fear**:
        - Irregular patterns with trembling effect
        - Variable amplitude with sudden changes
        - Higher frequency of micro-variations
        - Average RMS energy: ~0.38
        
        **Neutral**:
        - Most consistent and stable patterns
        - Moderate amplitude
        - Regular rhythm
        - Average RMS energy: ~0.35
        
        **Surprise**:
        - Sudden high-amplitude spikes
        - Sharp onset patterns
        - Quick transitions from low to high
        - Average RMS energy: ~0.48
        """)
    
    st.markdown("---")
    
    # Silence trimming visualization
    st.subheader("Recording Quality and Silence Removal")
    
    st.write("""
    Raw recordings contain silence before and after actual speech content. 
    This silence must be removed to improve feature quality.
    """)
    
    # IMAGE PLACEHOLDER 5: Figure 5 - Before/after trimming waveform
    st.image(IMG_DIR / "waveform_trim_comparison.png", 
            caption="Figure 5: Waveform of before and after trimming",
            use_container_width=True)
    
    with st.expander("Preprocessing Impact"):
        st.write("""
        ### Silence Trimming Process:
        
        **Original Waveform:**
        - Contains leading silence (0-0.5s typically)
        - Contains trailing silence (0.3-0.8s typically)
        - Silence can represent 10-30% of total duration
        
        **Trimmed Waveform:**
        - Speech content starts immediately
        - Minimal trailing silence (50ms padding)
        - Consistent speech segment boundaries
        
        **Benefits:**
        - Cleaner spectrograms without silent regions
        - More accurate duration measurements
        - Better alignment for batch processing
        - Improved feature extraction quality
        
        **Trimming Algorithm:**
        ```
        1. Calculate RMS energy per frame (frame_length=2048)
        2. Set threshold = max(energy) * 0.01 (-40 dB relative)
        3. Find first frame above threshold (start)
        4. Find last frame above threshold (end)
        5. Extract audio[start-padding : end+padding]
        ```
        """)
    
    st.markdown("---")
# ==================== TAB 4: FREQUENCY ANALYSIS ====================
with tab4:
    st.header("Determining Significant Frequency Ranges")
    
    st.write("""
    **Objective**: Identify frequency bands that exhibit the greatest discriminative power across 
    different emotional speech categories, informing optimal feature extraction and preprocessing strategies.
    """)
    
    st.markdown("---")
    
    # Methodology summary
    with st.expander("Analysis Methodology"):
        st.write("""
        ### Per-emotion average power computation
        
        For each emotion class e ‚àà {neutral, calm, happy, sad, angry, fearful, disgust, surprised}:
        
        - Transform samples into log-mel spectrograms S(t, f)
        - Calculate temporal average power: **PÃÖ‚Çë(f) = (1/T) Œ£‚Çú S(t,f)**
        - Aggregate across all samples in class e
        
        ### Inter-emotion variance analysis
        
        For each frequency bin f:
        
        - Construct vector: **Pf = [PÃÖ‚ÇÅ(f), PÃÖ‚ÇÇ(f), ..., PÃÖ‚Çà(f)]**
        - Compute variance: **Var(f) = (1/N) Œ£·µ¢ (PÃÖ·µ¢(f) - Œºf)¬≤**
        - High variance indicates meaningful spectral distinctions
        
        ### ANOVA-based significance testing
        
        - **H‚ÇÄ**: Mean spectral power at frequency f is equal across all emotions
        - **H‚ÇÅ**: At least one emotion has significantly different mean power
        - Compute F-statistic and p-value
        - Apply significance threshold **Œ± = 0.05**
        
        ### Contiguous range identification
        
        - Extract bins where p < 0.05
        - Merge bins separated by ‚â§ 100Hz
        - Filter ranges with bandwidth < 200Hz
        """)
    
    st.markdown("---")
    
    # Variance analysis
    st.subheader("Variance of Average Power")
    
    st.image(IMG_DIR / "variance_avg_power.png", 
            caption="Figure 8: Variance of average power across emotions",
            use_container_width=True)
    
    with st.expander("Variance Analysis Results"):
        st.write("""
        ### Key Observations:
        
        **Peak Variance Regions:**
        
        1. **0-800 Hz** (Fundamental Frequency & F1):
           - Highest variance spike at ~200-400 Hz
           - Variance: 16 dB¬≤
           - Captures pitch and first formant differences
           - Critical for distinguishing angry vs. sad
        
        2. **1500-3700 Hz** (Higher Formants):
           - Secondary variance peak at ~2500 Hz
           - Variance: 3-4 dB¬≤
           - Captures vocal resonance patterns
           - Important for happy vs. neutral distinction
        
        3. **4000-8000 Hz** (High Frequencies):
           - Lower variance (~1 dB¬≤)
           - Some discriminative value for angry emotion
           - Contains fricative and sibilant information
        
        **Low Variance Regions:**
        - 800-1500 Hz: Minimal emotion-specific variation
        - >8000 Hz: Negligible variance, mostly noise
        
        **Interpretation:**
        - Most emotion information concentrated below 4000 Hz
        - Low-to-mid frequency range (0-3700 Hz) is most discriminative
        - High frequencies contribute marginally beyond 5000 Hz
        """)
    
    st.markdown("---")
    
    # ANOVA results
    st.subheader("ANOVA Significance Testing")
    
    st.image(IMG_DIR / "anova_significance.png", 
            caption="Figure 9: ANOVA significance across frequencies",
            use_container_width=True)
    
    with st.expander("Statistical Significance Results"):
        st.write("""
        ### ANOVA Findings:
        
        **Highly Significant Ranges (p < 0.001):**
        
        - **200-800 Hz**: Strongest statistical significance
          - F-statistic: >100 in many bins
          - p-value: <10‚Åª‚Å∂
          - Fundamental frequency (F0) and first formant (F1) region
          - Captures pitch and vocal cord vibration differences
        
        - **1500-3700 Hz**: Moderate-strong significance
          - F-statistic: 20-60
          - p-value: <10‚Åª‚Å¥
          - Second and third formants (F2, F3)
          - Captures vocal tract resonance patterns
        
        **Moderately Significant (p < 0.05):**
        
        - **4000-5000 Hz**: Borderline significance
          - F-statistic: 5-15
          - p-value: 0.001-0.05
          - High-frequency vocal characteristics
          - Useful for some emotions (e.g., anger)
        
        **Not Significant (p ‚â• 0.05):**
        
        - **800-1500 Hz**: Gap region
        - **>8000 Hz**: Noise floor
        
        **Red line** (Œ± = 0.05): Frequencies below this line are statistically significant discriminators.
        
        **Conclusion**: Frequency ranges 200-800 Hz and 1500-3700 Hz provide the most reliable 
        emotion classification information.
        """)
    
    st.markdown("---")
    
    # Cumulative power
    st.subheader("Cumulative Discriminative Power")
    
    st.write("""
    A cumulative discriminative power curve highlights how much discriminative information is captured 
    below key cutoff frequencies (e.g., 2 kHz, 3.7 kHz, 8 kHz).
    """)
    
    st.image(IMG_DIR / "cumulative_discriminative_power.png", 
            caption="Figure 10: Cumulative discriminative power vs frequency",
            use_container_width=True)
    
    with st.expander("Cumulative Power Analysis"):
        st.write("""
        ### Discriminative Power Accumulation:
        
        The cumulative curve shows normalized discriminative score (1/(p+Œµ)) summed across frequency bins.
        
        **Key Cutoff Points:**
        
        | Frequency | Cumulative Power | Information Captured | Interpretation |
        |-----------|------------------|----------------------|----------------|
        | **2000 Hz** | ~50% | Half of discriminative info | F0 + F1 region |
        | **3740 Hz** | ~69-75% | Three-quarters captured | Includes F2, F3 |
        | **8000 Hz** | >90% | Nearly all information | Includes high freqs |
        | **12000 Hz** | ~100% | Complete (plateaus) | Minimal gain beyond |
        
        **Dataset Consistency:**
        
        Curves shown for individual datasets (RAVDESS, TESS, CREMA-D, SAVEE) and combined:
        
        - All datasets show similar cumulative patterns
        - Slight variations reflect recording quality differences
        - Combined curve (black) represents robust cross-dataset trend
        
        **Practical Implications:**
        
        1. **Frequency Cutoff Recommendation**: 8000 Hz captures >90% of emotion information
        2. **Minimum Viable Range**: 200-3740 Hz captures ~75% (sufficient for many applications)
        3. **Sampling Rate**: 16 kHz (Nyquist frequency 8 kHz) is adequate for SER
        4. **Feature Extraction**: Focus mel-spectrogram bins on 0-8000 Hz range
        
        **Why not use full bandwidth?**
        - >8 kHz provides <10% additional information
        - Higher frequencies mostly contain noise
        - Computational efficiency: narrower bandwidth = fewer features
        - Model generalization: reducing noise improves robustness
        """)
    
    st.markdown("---")
    
    # Summary of results
    st.subheader("Summary of Results")
    
    st.success("""
    ### Key Findings:
    
    **Primary Discriminative Ranges:**
    
    - **200-800 Hz** (Strongest): Fundamental pitch (F0) and first formant (F1)
    - **1500-3700 Hz** (Strong): Higher formants and vocal resonance patterns
    
    **Cross-Dataset Consensus:**
    
    - Most significant ranges: **0-3740 Hz**
    - Consistently important: **300-2800 Hz** (across all four datasets)
    
    **Cumulative Discriminative Power:**
    
    - **2000 Hz**: Captures ~50% of emotional information
    - **3700 Hz**: Captures 69-75% of information
    - **8000 Hz**: Captures over 90% of information
    
    **Conclusion:**
    
    The low to mid frequency ranges (200-3700 Hz) carry most emotional cues in human speech, 
    making them especially important for feature extraction in SER models.
    """)
    
    st.info("""
    **Practical Implications:**
    
    **Sampling Rate**: 16 kHz is sufficient (Nyquist frequency 8 kHz)  
    **Frequency Focus**: Emphasize 200-3700 Hz in feature extraction  
    **Mel-Spectrogram**: Configure bins to focus on this range  
    **Computational Efficiency**: Can safely low-pass filter at 8 kHz  
    """
)
# ==================== FOOTER ====================
st.markdown("---")
st.caption("Exploratory Data Analysis | Based on RAVDESS, TESS, CREMA-D, SAVEE datasets")