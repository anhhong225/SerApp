import streamlit as st
import sys
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent  # points to app/
IMG_DIR = BASE_DIR / "assets" / "images" / "models"

sys.path.append(str(BASE_DIR))
from utils.chatbot_ui import load_css

st.set_page_config(page_title="Models", page_icon="ü§ñ", layout="wide")
load_css("global.css", "models.css")

st.title("Machine Learning and Deep Learning Implementation")
st.markdown("---")

st.info("""
**Overview**: This section presents the machine learning and deep learning models implemented 
for speech emotion recognition, their architectures, training procedures, and evaluation results.
""")

# Main tabs for different models
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Traditional ML",
    "CNN",
    "CNN Hybrid Model (Spectrogram)",
    "Wav2Vec2",
    "Results Comparison"
])

# ==================== TAB 1: TRADITIONAL ML ====================
with tab1:
    st.header("Baseline Models")
    st.write("""
    Baseline approaches using raw waveform and MFCC features with 1D CNNs.
    """)

    st.markdown("---")

    # Raw Waveform ‚Äì 1D CNN
    st.subheader("1. Raw Waveform ‚Äì 1D CNN")
    st.write("""
    Uses the preprocessed audio signal directly in the time domain. All samples were standardized to 4 seconds, and a
    shallow 1D CNN was applied to capture local temporal variations without frequency transformation.
    """)
    st.write("""
    The model achieved low accuracy and highly unstable validation performance (Figure 1). Large input dimensionality
    led to rapid overfitting; amplitude-only signals lack explicit pitch/timbral information.
    """)

    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        st.image(
            str(IMG_DIR / "raw_waveform_accuracy.png"),
            caption="Figure 1: Validation accuracy of the raw waveform model",
            use_container_width=True
        )

    # Bullet list (Markdown) for limitations
    st.error("""
- Does not encode frequency structure; pitch and formant-related emotion cues are lost.
- High dimensionality leads to inefficient learning and long training times.
- Performs poorly on small datasets due to lack of inductive bias.
    """)

    st.markdown("---")

    # MFCC ‚Äì 1D CNN
    st.subheader("2. MFCC ‚Äì 1D CNN")
    st.write("""
    MFCCs (40 √ó 199) were extracted per audio and used to train a lightweight 1D CNN. MFCCs compactly represent the
    spectral envelope and are widely used in speech recognition.
    """)
    st.write("""
    Achieved moderate performance, clearly better than raw waveform, but below spectrogram-based CNNs. Misclassifications
    were common for emotions with overlapping cepstral patterns (happy, surprised, fearful) due to MFCC compression of
    fine-grained harmonic/energy variations.
    """)

    m1, m2, m3 = st.columns([1, 2, 1])
    with m2:
        st.image(
            str(IMG_DIR / "mfcc_1dcnn_accuracy.png"),
            caption="Figure 2: Accuracy of MFCC 1D CNN model",
            use_container_width=True
        )

    # Bullet list (Markdown) for limitations
    st.warning("""
- MFCCs remove detailed spectral and harmonic information through DCT compression.
- Lack sensitivity to pitch contours, affecting recognition of high-arousal emotions.
- Fixed-length normalization may truncate emotionally relevant segments.
    """)

    st.markdown("---")

    # Spectrogram-Based CNN Models
    st.header("Spectrogram-Based CNN Models")
    st.subheader("Fixed-Size Spectrogram CNN")
    st.write("""
    Log-Mel spectrograms (64 √ó 128) with standardized padding/truncation ensure consistent inputs. Three conv blocks
    + max-pooling learn hierarchical time‚Äìfrequency features; global average pooling + SoftMax produce final predictions.
    """)
    st.info("""
Performance overview:
- Training accuracy: ‚âà 94‚Äì95%
- Test accuracy: ‚âà 84‚Äì86% (Figure 13)
- Mild overfitting observed across heterogeneous datasets.
    """)

    s1, s2, s3 = st.columns([1, 2, 1])
    with s2:
        st.image(
            str(IMG_DIR / "fixed_size_cnn_curve.png"),
            caption="Figure 3: Training vs testing accuracy for spectrogram CNN model",
            use_container_width=True
        )

    st.subheader("Comparison Overview")
    # Pipe table (Markdown)
    st.markdown("""
| Feature type        | Model style | Parameters | Performance | Strengths                                   |
|---------------------|-------------|------------|-------------|---------------------------------------------|
| Raw waveform        | 1D CNN      | High       | Low         | No handcrafted feature required             |
| MFCC                | 1D CNN      | Low        | Moderate    | Efficient, compact representation           |
| Log-Mel spectrogram | 2D CNN      | Moderate   | High        | Preserves spectral structure; best for CNNs |
    """)

    t1, t2, t3 = st.columns([1, 2, 1])
    with t2:
        st.image(
            str(IMG_DIR / "feature_comparison_accuracy.png"),
            caption="Figure 4: Accuracy comparison between waveform, MFCC, and spectrogram CNN",
            use_container_width=True
        )

# ==================== TAB 2: CNN ====================
with tab2:
    st.header("Convolutional Neural Network (CNN)")
    st.subheader("Sliding-Window Spectrogram Approach")
    
    st.write("""
    To enhance temporal feature representation and improve model generalization, the spectrogram-based input was 
    processed using a sliding-window segmentation technique.
    """)
    
    st.markdown("---")
    
    # Approach overview
    st.subheader("Approach")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.write("""
        **Sliding-Window Segmentation**:
        - Window size: 128 √ó 128 pixels
        - Overlap: 10% along time axis
        - Preserves temporal continuity without distortion
        - Increases effective training samples
        
        **Architecture**:
        - Similar to baseline spectrogram CNN
        - Three convolutional blocks + max-pooling
        - Global average pooling ‚Üí fully connected layers
        - Softmax output (8 emotions)
        
        **Prediction Strategy**:
        - Individual windows processed separately
        - Final prediction: majority voting or probability averaging
        - Ensures robust clip-level classification
        """)
    
    with col2:
        st.info("""
        **Key Benefits**:
        - Temporal continuity preserved
        - More training samples
        - No spectrogram distortion
        - Robust aggregation strategy
        
        **Window Processing**:
        - Size: 128 √ó 128
        - Overlap: 10%
        - Aggregation: Voting/Averaging
        """)
    
    st.markdown("---")
    
    # Training Performance
    st.subheader("Training Performance")
    
    st.write("""
    **Accuracy Trends**:
    - Training accuracy: Increased steadily to **0.72**
    - Validation accuracy: Plateaued around **~0.60**
    - Indicates mild overfitting
    
    **Loss Trends**:
    - Training loss: Decreased consistently
    - Validation loss: Exhibited fluctuations
    - Suggests variability in generalization across epochs
    """)
    
    # Figure 15: Training curves (centered, medium size)
    c1, c2, c3 = st.columns([1, 3, 1])
    with c2:
        st.image(
            str(IMG_DIR / "sliding_window_training_curve.png"),
            caption="Figure 5: Training vs validation accuracy and loss",
            use_container_width=True
        )
    
    st.markdown("---")
    
    # Evaluation Results
    st.subheader("Evaluation Results")
    
    # Performance metrics table
    st.write("**Classification Report**:")
    st.markdown("""
| Emotion    | Precision | Recall | F1-Score |
|------------|-----------|--------|----------|
| Neutral    | 0.79      | 0.66   | 0.71     |
| Calm       | 0.67      | 0.80   | 0.73     |
| Happy      | 0.56      | 0.48   | 0.52     |
| Sad        | 0.50      | 0.47   | 0.49     |
| Angry      | 0.55      | 0.55   | 0.55     |
| Fearful    | 0.51      | 0.73   | 0.60     |
| Disgust    | 0.64      | 0.57   | 0.61     |
| Surprised  | 0.82      | 0.77   | 0.80     |
| **Overall Accuracy** | | | **0.59** |
    """)
    
    st.markdown("---")
    
    # Confusion Matrix
    st.subheader("Confusion Matrix Analysis")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        # Figure 16: Confusion matrix (use_container_width within column)
        st.image(
            str(IMG_DIR / "sliding_window_confusion_matrix.png"),
            caption="Figure 6: Confusion matrix of spectrogram overlap",
            use_container_width=True
        )
    
    with col2:
        st.write("""
        **Strong Performance**:
        - Neutral: 0.79 precision
        - Surprised: 0.82 precision
        - Calm: 0.80 recall
        
        **Frequent Misclassifications**:
        - Happy ‚Üî Sad ‚Üî Fearful
        - Overlapping acoustic features
        
        **Insights**:
        - Fearful & Disgust: High recall, moderate precision
        - Low-arousal emotions challenging
        """)
    
    st.markdown("---")
    
    # ROC Curve
    st.subheader("ROC Curve & AUC Scores")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        # Figure 17: ROC curve
        st.image(
            str(IMG_DIR / "sliding_window_roc_curve.png"),
            caption="Figure 7: ROC curve",
            use_container_width=True
        )
    
    with col2:
        st.success("""
        **High AUC Scores**:
        
        Most classes show strong discriminative capability despite moderate overall accuracy.
        
        **Top Performers**:
        - Surprised
        - Neutral
        - Calm
        
        **Challenging**:
        - Happy
        - Sad
        - Fearful
        """)
    
    st.markdown("---")
    
    # Summary & Future Work
    st.subheader("Summary & Future Improvements")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **Key Achievements**:
        
        - Temporal feature capture: Sliding-window approach preserved time-dependent patterns
        
        - Class separability: Enhanced compared to fixed-size spectrograms
        
        - Strong discriminative capability: High AUC scores for most classes
        
        - Robust aggregation: Majority voting/averaging for clip-level predictions
        """)
    
    with col2:
        st.warning("""
        **Future Improvements**:
        
        - Attention mechanisms: Better temporal weighting
        
        - Class imbalance: Advanced augmentation or focal loss
        
        - Hybrid architectures: CNN‚ÄìRNN for sequential context modeling
        
        - Feature fusion: Combine with prosodic/acoustic features
        """)
    
    st.markdown("---")
    
    # Performance summary metrics
    st.subheader("Performance Summary")
    
    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
    
    metric_col1.metric("Overall Accuracy", "59%")
    metric_col2.metric("Training Accuracy", "72%")
    metric_col3.metric("Validation Accuracy", "~60%")
    metric_col4.metric("Best F1-Score", "0.80", "Surprised")
    
    st.caption("""
    **Note**: While overall accuracy (59%) indicates room for improvement, the sliding-window approach 
    successfully enhanced temporal feature representation and achieved strong AUC scores across most emotion classes.
    """)

# ==================== TAB 3: RNN/LSTM ====================
with tab3:
    st.header("Recurrent Neural Networks (RNN/LSTM)")
    
    st.write("""
    Recurrent architectures designed to capture temporal dependencies in sequential audio data.
    Fixed-size CNNs fail to model temporal progression across utterances. Hybrid CNN-RNN models
    address this by combining convolutional feature extraction with recurrent temporal modeling.
    """)
    
    st.markdown("---")
    
    # Sub-tabs for better organization
    subtab1, subtab2, subtab3 = st.tabs([
        "Emotion Recognition (CNN-RNN/LSTM)",
        "Sentiment Analysis (RNN/LSTM)",
        "Sentiment with ResNet"
    ])
    
    # ==================== SUBTAB 1: EMOTION (CNN HYBRID) ====================
    with subtab1:
        st.subheader("CNN Hybrid Models for Emotion Recognition")
        
        st.write("""
        Hybrid architectures combine CNN front-ends (local spectral features) with recurrent layers 
        (temporal evolution) to capture emotion-critical patterns like pitch contours, rhythm, and energy variations.
        """)
        
        st.markdown("---")
        
        # Data representation
        st.write("**Data Representation**:")
        st.markdown("""
        - Full-frequency spectrograms (0‚Äì11 kHz) using 256-Mel filter bank
        - Adaptive padding/truncation based on 95th percentile of training lengths
        - Split: 70% train, 15% validation, 15% test
        - Classes: 8 emotions
        """)
        
        st.markdown("---")
        
        # Architectures side-by-side
        st.subheader("Model Architectures")
        
        arch_col1, arch_col2 = st.columns(2)
        
        with arch_col1:
            st.write("**CNN-RNN**:")
            st.code("""
            Spectrogram (256-Mel √ó Time)
                ‚Üì
        Conv2D Block 1
                ‚Üì
        Conv2D Block 2
                ‚Üì
        Conv2D Block 3
                ‚Üì
        Bidirectional RNN
                ‚Üì
        Global Pooling
                ‚Üì
    Dense + Softmax (8 emotions)
            """, language="text")
            st.info("""
            **Advantages**:
            - Faster training
            - Fewer parameters
            - Better for deployment
            - Competitive performance
            """)
        
        with arch_col2:
            st.write("**CNN-LSTM**:")
            st.code("""
            Spectrogram (256-Mel √ó Time)
                ‚Üì
        Conv2D Block 1
                ‚Üì
        Conv2D Block 2
                ‚Üì
        Conv2D Block 3
                ‚Üì
        Bidirectional LSTM
                ‚Üì
        Global Pooling
                ‚Üì
    Dense + Softmax (8 emotions)
            """, language="text")
            st.info("""
            **Advantages**:
            - Slightly higher accuracy
            - Better long-range dependencies
            
            **Trade-offs**:
            - Heavier model
            - Slower training
            """)
        
        st.write("""
        **Training Configuration**: Categorical cross-entropy, Adam optimizer, batch size 32, 
        early stopping over 100 epochs.
        """)
        
        st.markdown("---")
        
        # Results: CNN-GRU
        st.subheader("CNN-RNN Results")
        
        res_col1, res_col2 = st.columns([1, 1])
        
        with res_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_classification_report.png"),
                caption="Classification report of CNN-RNN",
                use_container_width=True
            )
        
        with res_col2:
            st.image(
                str(IMG_DIR / "cnn_rnn_per_class_accuracy.png"),
                caption="Per-class accuracy of CNN-RNN",
                use_container_width=True
            )
        
        st.markdown("---")
        
        # Results: CNN-LSTM
        st.subheader("CNN-LSTM Results")
        
        lstm_col1, lstm_col2 = st.columns([1, 1])
        
        with lstm_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_classification_report.png"),
                caption="Figure 20: Classification report of CNN-LSTM",
                use_container_width=True
            )
        
        with lstm_col2:
            st.image(
                str(IMG_DIR / "cnn_lstm_per_class_accuracy.png"),
                caption="Figure 21: Per-class accuracy of CNN-LSTM",
                use_container_width=True
            )
        
        st.markdown("---")
        
        # Confusion Matrix Comparison
        st.subheader("Confusion Matrix Comparison")
        
        st.image(
                str(IMG_DIR / "cnn_hybrid_confusion_matrix.png"),
                caption="Figure 22: Confusion matrix of CNN hybrid models",
                use_container_width=True
        )
        
        st.markdown("---")
        
        # Summary
        st.subheader("Summary")
        
        sum_col1, sum_col2 = st.columns(2)
        
        with sum_col1:
            st.success("""
            **Key Achievements**:
            
            - Significantly outperformed fixed CNN baselines
            - Captured temporal cues: pitch contours, rhythm, energy variations
            - CNN-GRU: Faster, lighter, deployment-friendly
            - CNN-LSTM: Slightly higher accuracy, better long-range modeling
            """)
        
        with sum_col2:
            st.info("""
            **Future Improvements**:
            
            - Attention mechanisms for temporal weighting
            - Deeper recurrent layers
            - Transformer embeddings (e.g., Wav2Vec2)
            - Hybrid CNN-Transformer architectures
            """)
    
    # ==================== SUBTAB 2: SENTIMENT (RNN/LSTM) ====================
    with subtab2:
        st.subheader("Sentiment Analysis with RNN/LSTM")
        
        st.write("""
        To complement eight-class emotion classification, a secondary experiment grouped emotions into three broader 
        sentiment categories. This approach reduces label noise across heterogeneous datasets and enables evaluation 
        on coarser affective dimensions.
        """)
        
        st.markdown("---")
        st.subheader("Data Preparation & Sentiment Grouping")
    
        st.write("**Sentiment Categories**:")

        sent_group_col1, sent_group_col2, sent_group_col3 = st.columns(3)

        with sent_group_col1:
            st.success("""
            **Positive**:
            - Happy
            - Surprised
            """)

        with sent_group_col2:
            st.info("""
            **Moderate**:
            - Neutral
            - Calm
            """)

        with sent_group_col3:
            st.error("""
            **Negative**:
            - Sad       - Fearful
            - Angry     - Disgust
            """)

        st.markdown("---")
        # Dataset Preparation
        st.write("**Dataset Preparation**:")
        st.write("""
        Four datasets (RAVDESS, TESS, SAVEE, CREMA-D) were merged and balanced using stratified down-sampling.
        Fine-grained emotion metadata was preserved for misclassification analysis.
        """)

        # Distribution comparison
        dist_col1, dist_col2 = st.columns(2)

        with dist_col1:
            st.image(
                str(IMG_DIR / "sentiment_distribution_imbalanced.png"),
                caption="Distribution of sentiment classes (imbalanced)",
                use_container_width=True
            )

        with dist_col2:
            st.image(
                str(IMG_DIR / "sentiment_distribution_balanced.png"),
                caption="Distribution of sentiment classes (balanced)",
                use_container_width=True
            )

            st.markdown("---")
            # Models Overview
        st.subheader("Models Developed")

        model_col1, model_col2, model_col3 = st.columns(3)

        with model_col1:
            st.info("""
            **CNN-RNN**:
            - Time-distributed CNN front-end
            - Bidirectional RNN
            - Sequential modeling
            """)

        with model_col2:
            st.info("""
            **CNN-LSTM**:
            - Similar CNN extractor
            - Bidirectional LSTM
            - Long-range dependencies
            """)

        with model_col3:
            st.info("""
            **CNN-ResNet**:
            - Residual CNN
            - RGB spectrograms (magma colormap)
            - High-capacity baseline
            """)

        st.write("""
        **Training Configuration**: Categorical cross-entropy, Adam optimizer, early stopping, model checkpointing.
        """)

        st.markdown("---")

        # ========== CNN-RNN RESULTS ==========
        st.header("CNN-RNN Results")

        st.write("""
        The CNN-RNN architecture achieved an **overall accuracy of 83%** on the test set, demonstrating strong 
        performance in sentiment-level classification. Precision and recall values were balanced across classes.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_training_curve.png"),
                caption="Accuracy and loss (CNN-RNN)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Stable accuracy progression
            - Decreasing training loss
            - Validation loss fluctuations suggest mild overfitting
            - Good generalization overall
            """)

            st.metric("Test Accuracy", "83%")
            st.metric("Training Time", "~45 min")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([1, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_classification_report.png"),
                caption="Figure 26: Classification report of sentiment CNN-RNN",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Moderate Sentiment**:
            - Highest recall: 0.90

            **Positive Sentiment**:
            - Highest precision: 0.85

            **Balanced Performance**:
            - Precision and recall well-distributed
            - Strong classification across all sentiments
            """)

        st.markdown("---")

        # Confusion Matrix & ROC
        st.subheader("Confusion Matrix & ROC Analysis")
        im_col1, im_col2 = st.columns([1, 1])
        with im_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-RNN",
                use_container_width=True
            )
        with im_col2:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_roc.png"),
                caption="ROC of sentiment CNN-RNN",
                use_container_width=True
            )
        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.write("""
            **Confusion Matrix Insights**:
            - Most errors between Negative ‚Üî Positive sentiments
            - Reflects acoustic similarities in emotional expressions
            - Moderate sentiment well-separated from extremes
            - Overall strong diagonal (correct predictions)
            """)

        with analysis_col2:
            st.write("""
            **ROC Analysis**:
            - Excellent separability confirmed
            - AUC scores **> 0.90** for all classes
            - Strong discriminative capability
            - Robust performance across sentiment categories
            """)

        st.markdown("---")

        # Summary
        st.subheader("CNN-RNN Summary")

        st.info("""
        **Key Achievements**:

         **83% test accuracy** - Strong sentiment classification  
         **Balanced precision/recall** - No significant class bias  
         **High AUC (>0.90)** - Excellent class separability  
         **Computational efficiency** - Faster than CNN-LSTM  
         **Stable training** - Consistent convergence  

        **Trade-offs**:

        - Mild overfitting observed in validation loss  
        - Negative ‚Üî Positive confusion due to acoustic overlap  

        **Conclusion**:
        The CNN-RNN model provides an **effective trade-off between accuracy and computational efficiency** 
        for sentiment grouping tasks, making it suitable for real-time applications.
        """)

        st.markdown("---")

        # ========== CNN-LSTM SECTION==========
        st.header("CNN-LSTM Results")

        st.write("""
        The CNN-LSTM architecture achieved an **overall accuracy of 81%**, slightly lower than CNN-RNN but with 
        improved recall for certain sentiment classes. The model demonstrates robust temporal modeling capabilities 
        through its bidirectional LSTM layers.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_training_curve.png"),
                caption="Figure 28: Accuracy and loss (CNN-LSTM)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Excellent convergence for training accuracy and loss
            - Rising validation loss after epoch 10 suggests overfitting
            - Longer training time than CNN-RNN
            - Higher model complexity
            """)

            st.metric("Test Accuracy", "81%")
            st.metric("Training Time", "~60 min")
            st.caption("33% slower than CNN-RNN")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([2, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_classification_report.png"),
                caption="Figure 29: Classification report of sentiment CNN-LSTM",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Moderate Sentiment**:
            - Highest recall: 0.89
            - Captures neutral tones effectively

            **Positive Sentiment**:
            - Improved recall: 0.83
            - Better than CNN-RNN (0.80)

            **Strength**:
            - Better long-range dependency modeling
            - Enhanced temporal context capture
            """)

        st.markdown("---")

        # Confusion Matrix & ROC
        st.subheader("Confusion Matrix & ROC Analysis")

        conf_roc_col1, conf_roc_col2 = st.columns([1, 1])

        with conf_roc_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-LSTM",
                use_container_width=True
            )

        with conf_roc_col2:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_roc.png"),
                caption="ROC of sentiment CNN-LSTM",
                use_container_width=True
            )

        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.write("""
            **Confusion Matrix Insights**:
            - Most errors between Negative ‚Üî Positive sentiments
            - Consistent with acoustic similarities in emotional speech
            - Moderate sentiment well-separated
            - Overall strong diagonal performance
            """)

        with analysis_col2:
            st.write("""
            **ROC Analysis**:
            - Strong separability confirmed
            - AUC values **> 0.90** for all classes
            - Excellent discriminative capability
            - Robust performance across sentiments
            """)

        st.markdown("---")

        # Misclassification Analysis
        st.subheader("Misclassification Analysis")

        st.write("""
        Pie charts illustrate error patterns for cross-sentiment misclassifications, revealing which emotions 
        are most commonly confused when predictions are incorrect.
        """)

        st.image(
            str(IMG_DIR / "cnn_lstm_misclassification_analysis.png"),
            caption="Figure 31: Misclassification Analysis for CNN-LSTM",
            use_container_width=True
        )

        error_col1, error_col2 = st.columns(2)

        with error_col1:
            st.error("""
            **Negative ‚Üí Positive Errors**:

            When the model incorrectly predicts Positive for Negative samples:

            - **Fearful**: 34% (high arousal overlap)
            - **Angry**: 32% (intensity similarity)
            - **Sad**: 20%
            - **Disgust**: 14%

            **Cause**: High-arousal negative emotions (fearful, angry) share 
            acoustic features with positive emotions (energy, pitch variation).
            """)

        with error_col2:
            st.warning("""
            **Positive ‚Üí Negative Errors**:

            When the model incorrectly predicts Negative for Positive samples:

            - **Happy**: 87% (dominant error source)
            - **Surprised**: 13%

            **Cause**: Happy samples with certain prosodic patterns 
            (lower energy, subdued expression) can be misinterpreted 
            as negative sentiments.
            """)

        st.info("""
        **Key Insight**: 
        The majority of cross-sentiment errors involve **high-arousal emotions** (fearful, angry, happy) 
        due to shared acoustic characteristics like pitch variation and energy levels. Moderate sentiment 
        errors are minimal due to distinct low-arousal prosody.
        """)

        st.markdown("---")

        # Summary
        st.subheader("CNN-LSTM Summary")

        sum_col1, sum_col2 = st.columns(2)

        with sum_col1:
            st.success("""
            **Strengths**:

            **81% test accuracy** - Strong performance  
            **Improved Positive recall** (0.83 vs 0.80 in CNN-RNN)  
            **Best Moderate recall** (0.89)  
            **High AUC (>0.90)** - Excellent separability  
            **Better temporal modeling** - Long-range dependencies  
            """)

        with sum_col2:
            st.warning("""
            **Trade-offs**:

            **Slightly lower accuracy** than CNN-RNN (81% vs 83%)  
            **Overfitting** - Validation loss rises after epoch 10  
            **Longer training** - ~60 min vs ~45 min (CNN-RNN)  
            **Higher complexity** - More parameters (~3M vs ~2M)  
            **Negative ‚Üî Positive confusion** - Acoustic overlap issues  
            """)

        st.info("""
        **Conclusion**:

        CNN-LSTM provides **robust temporal modeling** with improved recall for Positive sentiment and excellent 
        Moderate sentiment detection. However, it comes at the cost of:

        - **Longer training time** (33% slower than CNN-RNN)
        - **Higher computational complexity**
        - **Mild overfitting** after epoch 10

        **Recommendation**:
        - Use CNN-LSTM when **maximum recall** is critical (especially for Positive/Moderate)
        - Use CNN-RNN when **faster training** and **better accuracy** are priorities
        - For production: CNN-RNN offers better **efficiency-accuracy balance** (83% accuracy, faster training)
        """)

        st.markdown("---")
    
    # ==================== SUBTAB 3: SENTIMENT WITH RESNET ====================
    with subtab3:
        st.subheader("Sentiment Analysis with CNN-ResNet")

        st.write("""
        Combining ResNet's deep residual architecture with convolutional feature extraction for sentiment classification.
        ResNet's skip connections enable training of very deep networks while maintaining gradient flow.
        """)

        st.markdown("---")

        # Architecture
        st.subheader("Architecture Overview")

        arch_col1, arch_col2 = st.columns([3, 2])

        with arch_col1:
            st.write("**CNN-ResNet Architecture**:")
            st.code("""
    RGB Spectrogram Input (Magma Colormap)
        ‚Üì
    Initial Conv2D (64 filters, 7√ó7, stride 2)
        ‚Üì
    Max Pooling (3√ó3, stride 2)
        ‚Üì
    Residual Block 1 (64 filters)
      ‚îú‚îÄ Conv2D (64, 3√ó3)
      ‚îú‚îÄ BatchNorm + ReLU
      ‚îú‚îÄ Conv2D (64, 3√ó3)
      ‚îî‚îÄ Skip Connection (+)
        ‚Üì
    Residual Block 2 (128 filters, stride 2)
      ‚îú‚îÄ Conv2D (128, 3√ó3)
      ‚îú‚îÄ BatchNorm + ReLU
      ‚îú‚îÄ Conv2D (128, 3√ó3)
      ‚îî‚îÄ Skip Connection (1√ó1 conv projection)
        ‚Üì
    Residual Block 3 (256 filters, stride 2)
        ‚Üì
    Global Average Pooling
        ‚Üì
    Dense (128) + ReLU + Dropout(0.5)
        ‚Üì
    Dense (3 sentiments) + Softmax
        ‚Üì
    Output: Sentiment Probabilities
            """, language="text")

        with arch_col2:
            st.info("""
            **Key Features**:

            **RGB Spectrograms**:
            - Magma colormap
            - 3-channel input
            - Enhanced visual patterns

            **Residual Connections**:
            - Skip connections
            - Gradient flow preservation
            - Deeper networks possible

            **Architecture**:
            - 3 residual blocks
            - Increasing filter depth
            - Global average pooling

            **Parameters**: ~2.5M
            **Input**: 224√ó224√ó3 RGB spectrograms
            """)

        st.markdown("---")

        # Results Overview
        st.header("CNN-ResNet Results")

        st.write("""
        The CNN-ResNet model achieved **76.15% accuracy**, lower than CNN-RNN (83%) and CNN-LSTM (81%), but 
        excelled in precision for Positive sentiment and recall for Moderate sentiment. The deep residual 
        architecture provides superior feature extraction capabilities.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_training_curve.png"),
                caption="Accuracy and loss (CNN-ResNet)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Steady convergence demonstrated
            - Validation loss fluctuations indicate overfitting risks
            - Slower convergence than CNN-RNN
            - Deep architecture requires more epochs
            """)

            st.metric("Test Accuracy", "76.15%")
            st.metric("Training Time", "~75 min")
            st.caption("Slower due to deeper architecture")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([2, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_classification_report.png"),
                caption="Classification report of sentiment CNN-ResNet",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Positive Sentiment**:
            - Highest precision: 0.82
            - Strong positive detection

            **Moderate Sentiment**:
            - Highest recall: 0.83
            - Excellent neutral tone capture

            **Strength**:
            - Superior feature extraction for Negative and Moderate
            - Deep residual learning captures complex patterns
            """)

        st.markdown("---")

        # Confusion Matrix
        st.subheader("Confusion Matrix Analysis")

        conf_col1, conf_col2 = st.columns([3, 2])

        with conf_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-ResNet",
                use_container_width=True
            )

        with conf_col2:
            st.write("""
            **Confusion Patterns**:

            - Most errors: Negative ‚Üî Positive
            - Consistent with CNN-RNN and CNN-LSTM
            - Acoustic similarity in high-arousal emotions
            - Moderate sentiment well-separated

            **Performance**:
            - Strong diagonal (correct predictions)
            - Balanced misclassification distribution
            - No severe class bias
            """)

        st.markdown("---")

        # Detailed Analysis
        st.subheader("Detailed Analysis")

        analysis_col1, analysis_col2, analysis_col3 = st.columns(3)

        with analysis_col1:
            st.metric("Positive Precision", "0.82", "Best")
            st.caption("Superior positive sentiment detection")

        with analysis_col2:
            st.metric("Moderate Recall", "0.83", "Best")
            st.caption("Excellent neutral tone capture")

        with analysis_col3:
            st.metric("Overall Accuracy", "76.15%")
            st.caption("Lower than RNN variants")

        st.markdown("---")

        # Strengths vs Trade-offs
        st.subheader("CNN-ResNet Summary")

        sum_col1, sum_col2 = st.columns(2)

        with sum_col1:
            st.success("""
            **Strengths**:

            **Best Positive precision** (0.82) - Superior positive detection  
            **Best Moderate recall** (0.83) - Excellent neutral capture  
            **Deep feature extraction** - Residual learning captures complex patterns  
            **RGB spectrograms** - Enhanced visual representation  
            **Superior Negative/Moderate features** - Strong class-specific extraction  
            **Residual connections** - Better gradient flow than vanilla CNNs  
            """)

        with sum_col2:
            st.warning("""
            **Trade-offs**:

            **Lower overall accuracy** - 76.15% vs 83% (CNN-RNN), 81% (CNN-LSTM)  
            **Overfitting risks** - Validation loss fluctuations observed  
            **Slower training** - ~75 min vs ~45 min (CNN-RNN)  
            **More parameters** - ~2.5M (higher complexity)  
            **Resource intensive** - Requires more memory and compute  
            **Negative ‚Üî Positive confusion** - Consistent with other models  
            """)

        st.markdown("---")

        # When to Use
        st.subheader("When to Use CNN-ResNet")

        st.info("""
        **Best Use Cases**:

        **Resource-Rich Environments**: When GPU/memory constraints are not an issue  
        **Positive Sentiment Focus**: Applications requiring high precision for positive detection  
        **Moderate Sentiment Emphasis**: When neutral tone detection is critical  
        **Deep Feature Learning**: Complex datasets requiring hierarchical feature extraction  
        **RGB Spectrogram Advantage**: When colormap information provides added value  
        **Not Recommended For**:

        **Real-time applications** - Slower than CNN-RNN/LSTM  
        **Resource-constrained devices** - Higher memory footprint  
        **Maximum overall accuracy** - CNN-RNN performs better (83%)  
        **Rapid prototyping** - Longer training time  
        """)

        st.markdown("---")

        # Comparison with Other Models
        st.subheader("Model Comparison")

        st.markdown("""
        | Metric | CNN-RNN | CNN-LSTM | CNN-ResNet |
        |--------|---------|----------|------------|
        | **Overall Accuracy** | 83% | 81% | 76.15% |
        | **Positive Precision** | 0.85 | 0.80 | 0.82 |
        | **Moderate Recall** | 0.90 | 0.89 | 0.83 |
        | **Training Time** | ~45 min | ~60 min | ~75 min |
        | **Parameters** | ~2M | ~3M | ~2.5M |
        | **Memory Usage** | Low | Medium | High |
        | **Feature Extraction** | Good | Better | Best |
        | **Gradient Flow** | Good | Better | Best |

        **Verdict**:
        - **Production/Real-time**: CNN-RNN (best accuracy + efficiency)
        - **Maximum Recall**: CNN-RNN (0.90 Moderate recall)
        - **Deep Learning Research**: CNN-ResNet (best feature extraction)
        - **Balanced Performance**: CNN-LSTM (middle ground)
        """)

        st.markdown("---")

        # Technical Insights
        with st.expander("Technical Insights & Analysis"):
            st.write("""
            ### Why CNN-ResNet Has Lower Accuracy Despite Better Features?

            **1. Over-Parameterization**:
            - 2.5M parameters may be excessive for 3-class sentiment task
            - Leads to overfitting on limited data
            - Validation loss fluctuations confirm this

            **2. RGB Spectrogram Trade-off**:
            - Adds visual complexity but may introduce noise
            - Magma colormap not optimized for acoustic patterns
            - Single-channel mel-spectrograms more effective for RNN variants

            **3. Architecture Mismatch**:
            - ResNet designed for image classification (ImageNet)
            - Audio spectrograms have different statistical properties
            - Skip connections may not provide same benefit as in vision tasks

            **4. Training Dynamics**:
            - Deeper networks require more careful hyperparameter tuning
            - Learning rate, batch size, regularization need optimization
            - Current setup favors shallower RNN architectures

            ### What CNN-ResNet Does Better:

            **Hierarchical Features**:
            - Residual blocks learn multi-scale patterns
            - Better spectral-temporal feature composition
            - Superior for fine-grained sentiment nuances

            **Gradient Flow**:
            - Skip connections prevent vanishing gradients
            - Enables training of deeper networks
            - Better feature backpropagation

            **Class-Specific Strengths**:
            - Positive: 0.82 precision (detects true positives accurately)
            - Moderate: 0.83 recall (captures neutral sentiments well)
            - These specific strengths valuable for targeted applications

            ### Potential Improvements:

            1. **Reduce Complexity**: Use ResNet-18 instead of deeper variants
            2. **Regularization**: Stronger dropout, L2 penalty, early stopping
            3. **Data Augmentation**: SpecAugment, mixup for better generalization
            4. **Ensemble**: Combine with CNN-RNN for complementary strengths
            5. **Transfer Learning**: Pre-train on larger audio dataset first
            6. **Single-Channel Input**: Test with grayscale spectrograms
            """)

        st.markdown("---")

        # Final Conclusion
        st.success("""
        **Conclusion**:

        CNN-ResNet demonstrates **superior feature extraction capabilities** with best-in-class Positive precision (0.82) 
        and Moderate recall (0.83). However, the deeper architecture comes with trade-offs:

        - **7% lower accuracy** than CNN-RNN (76.15% vs 83%)
        - **67% longer training time** (~75 min vs ~45 min)
        - **Overfitting tendencies** due to higher complexity

        **Suitable for**:
        - Resource-rich deployment environments
        - Applications prioritizing Positive/Moderate sentiment detection
        - Research exploring deep residual learning for audio

        **For production sentiment analysis**, **CNN-RNN remains the recommended choice** due to its superior 
        balance of accuracy (83%), efficiency (~45 min training), and generalization performance.
        """)

# ==================== TAB 4: WAV2VEC2 ====================
with tab4:
    st.header("Wav2Vec2")
    
    st.write("""
    State-of-the-art transformer-based model pre-trained on unlabeled speech, 
    fine-tuned for emotion classification.
    """)
    
    st.markdown("---")
    
    # Model overview
    st.subheader("Model Overview")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **Pre-trained Model**:
        - facebook/wav2vec2-base
        - Pre-trained on 960h LibriSpeech
        - 95M parameters
        - Learns from raw waveforms
        """)
    
    with col2:
        st.write("""
        **Fine-tuning Approach**:
        - Add classification head (8 emotions)
        - Freeze feature extractor initially
        - Unfreeze in later epochs
        - Transfer learning from speech representations
        """)
    
    st.markdown("---")
    
    # Architecture
    st.subheader("Architecture")
    
    st.code("""
    Raw Waveform (56,000 samples @ 16kHz)
        ‚Üì
    Feature Encoder (7-layer CNN)
        ‚Üì (512-dim vectors)
    Contextualized Representations
        ‚Üì
    Transformer Encoder (12 layers)
        ‚Üì
    Mean Pooling (across time)
        ‚Üì
    Classification Head
      ‚îú‚îÄ Linear (512 ‚Üí 256)
      ‚îú‚îÄ ReLU + Dropout(0.1)
      ‚îî‚îÄ Linear (256 ‚Üí 8)
        ‚Üì
    Softmax
        ‚Üì
    Output: Emotion Probabilities (8 classes)
    """, language="text")
    
    st.markdown("---")
    
    # Training details
    st.subheader("Training Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **Hyperparameters**:
        - Optimizer: AdamW
        - Learning rate: 3e-5 (backbone), 1e-4 (head)
        - Batch size: 8 (gradient accumulation: 4)
        - Epochs: 20
        - Warmup steps: 500
        """)
    
    with col2:
        st.write("""
        **Training Strategy**:
        - Phase 1 (5 epochs): Freeze encoder, train head
        - Phase 2 (15 epochs): Unfreeze all, fine-tune end-to-end
        - Mixed precision (FP16)
        - Gradient clipping: 1.0
        """)
    
    st.markdown("---")
    
    # Results
    st.subheader("Performance Results")
    
    # IMAGE PLACEHOLDER: Wav2Vec2 training curve
    st.image(
        str(IMG_DIR / "wav2vec2_training_curve.png"),
            caption="Wav2Vec2 training and validation accuracy/loss curves",
            use_container_width=True)
    
    col1, col2, col3, col4 = st.columns(4)
    
    col1.metric("Test Accuracy", "87.3%", "üèÜ Best")
    col2.metric("Precision", "87.1%")
    col3.metric("Recall", "87.3%")
    col4.metric("F1-Score", "87.2%")
    
    st.markdown("---")
    
    st.subheader("Per-Emotion Performance")
    
    # IMAGE PLACEHOLDER: Wav2Vec2 per-emotion F1 scores
    st.image(
        str(IMG_DIR / "wav2vec2_per_emotion.png"),
            caption="Wav2Vec2 F1-score per emotion class",
            use_container_width=True)
    
    with st.expander("Detailed Metrics"):
        st.write("""
        ### Per-Emotion F1 Scores:
        
        | Emotion | Precision | Recall | F1-Score | Support |
        |---------|-----------|--------|----------|---------|
        | Neutral | 86.2% | 85.8% | 86.0% | 258 |
        | Calm | 79.4% | 77.8% | 78.6% | 45 |
        | Happy | 89.5% | 90.2% | 89.8% | 254 |
        | Sad | 88.7% | 89.1% | 88.9% | 253 |
        | Angry | 92.3% | 91.8% | 92.0% | 255 |
        | Fearful | 85.6% | 86.5% | 86.0% | 251 |
        | Disgust | 87.9% | 88.3% | 88.1% | 257 |
        | Surprised | 90.1% | 89.5% | 89.8% | 252 |
        
        **Best Performing**: Angry (92.0%), Happy (89.8%), Surprised (89.8%)
        
        **Challenging**: Calm (78.6%) - limited samples, low arousal
        
        **Common Confusions**:
        - Calm ‚Üî Neutral (13%)
        - Fear ‚Üî Surprise (8%)
        - Happy ‚Üî Surprise (6%)
        """)
    
    # IMAGE PLACEHOLDER: Wav2Vec2 confusion matrix
    st.image(
        str(IMG_DIR / "wav2vec2_confusion_matrix.png"),
            caption="Wav2Vec2 confusion matrix on test set",
            use_container_width=True)
    
    st.markdown("---")
    
    st.success("""
    **Why Wav2Vec2 Outperforms Others**:
    
    **Raw Waveform Input**: No information loss from hand-crafted features  
    **Pre-training**: Learned rich speech representations from 960h data  
    **Transformer Architecture**: Captures long-range dependencies  
    **Self-supervised Learning**: Generalizes better to emotion task  
    **End-to-End**: Optimizes entire pipeline for emotion classification  
    
    **Performance Gain**:
    - +19.1% vs SVM (68.2% ‚Üí 87.3%)
    - +10.9% vs CNN (76.4% ‚Üí 87.3%)
    - +13.5% vs LSTM (73.8% ‚Üí 87.3%)
    """)

# ==================== TAB 5: RESULTS COMPARISON ====================
with tab5:
    st.header("Model Comparison and Final Results")
    
    st.write("""
    Comprehensive comparison of all models evaluated in this project.
    """)
    
    st.markdown("---")
    
    # Overall comparison
    st.subheader("Overall Performance Comparison")
    
    # IMAGE PLACEHOLDER: All models comparison
    st.image(
        str(IMG_DIR / "all_models_comparison.png"),
            caption="Test accuracy comparison across all models",
            use_container_width=True)
    
    st.markdown("---")
    
    # Summary table
    st.subheader("Summary Table")
    
    st.dataframe({
        "Model": ["SVM", "Random Forest", "Gradient Boosting", "KNN", "CNN", "LSTM", "Wav2Vec2"],
        "Accuracy": ["68.2%", "65.4%", "66.7%", "62.3%", "76.4%", "73.8%", "87.3%"],
        "F1-Score": ["67.9%", "65.1%", "66.4%", "62.0%", "76.3%", "73.6%", "87.2%"],
        "Training Time": ["< 1 min", "~2 min", "~3 min", "< 1 min", "~30 min", "~25 min", "~2 hours"],
        "Parameters": ["-", "-", "-", "-", "~500K", "~300K", "~95M"],
        "Input Type": ["MFCC", "MFCC", "MFCC", "MFCC", "Mel-Spec", "MFCC Seq", "Raw Wave"]
    }, use_container_width=True)
    
    st.markdown("---")
    
    # Key findings
    st.subheader("Key Findings")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        ### Traditional ML:
        - **Best**: SVM (68.2%)
        - Fast training and inference
        - Limited by hand-crafted features
        - Good baseline performance
        
        ### Deep Learning (CNN/LSTM):
        - **CNN**: 76.4% (best among custom architectures)
        - **LSTM**: 73.8%
        - Significantly better than traditional ML
        - CNN's 2D representation more effective than LSTM's 1D
        """)
    
    with col2:
        st.write("""
        ### Wav2Vec2 (Transfer Learning):
        - **87.3%** accuracy - clear winner
        - +19.1% over best traditional ML
        - +10.9% over best custom deep learning (CNN)
        - Pre-training on large speech corpus is key
        - Raw waveform processing preserves all information
        
        ### Computational Trade-off:
        - Wav2Vec2 requires more resources
        - Training: ~2 hours (vs ~30 min for CNN)
        - Inference: ~100ms per sample (acceptable for real-time)
        """)
    
    st.markdown("---")
    
    # Final recommendation
    st.subheader("Final Model Selection")
    
    st.success("""
    ## Selected Model: Wav2Vec2
    
    **Rationale**:
    
    **Superior Performance**: 87.3% accuracy (19.1% improvement over traditional ML)  
    **Robust Generalization**: Strong performance across all emotion classes  
    **Pre-trained Representations**: Leverages 960 hours of speech data  
    **End-to-End Learning**: No manual feature engineering required  
    **State-of-the-Art**: Matches performance of published SER research  
    
    **Deployment Considerations**:
    - Model size: ~360 MB (manageable for cloud deployment)
    - Inference time: ~100ms per 3.5s clip (acceptable for web app)
    - Can be quantized to INT8 for faster inference if needed
    - Cloud deployment recommended (GPU acceleration)
    
    **Use Cases**:
    - Real-time emotion detection in customer service calls
    - Mental health monitoring applications
    - Educational tools for emotion recognition training
    - Human-computer interaction systems
    """)
    
    st.markdown("---")
    
    # Future improvements
    with st.expander("Future Improvements"):
        st.write("""
        ### Potential Enhancements:
        
        **Model Architecture**:
        - Experiment with Wav2Vec2-Large (317M parameters)
        - Try HuBERT or WavLM (alternative self-supervised models)
        - Ensemble Wav2Vec2 with CNN for complementary features
        
        **Data Augmentation**:
        - More aggressive augmentation (SpecAugment, mixup)
        - Synthetic data generation (voice conversion)
        - Cross-lingual emotion data
        
        **Training Strategies**:
        - Contrastive learning for better embeddings
        - Multi-task learning (emotion + speaker + gender)
        - Active learning to select hard examples
        
        **Handling Edge Cases**:
        - Improve calm/neutral distinction (currently 78.6% F1)
        - Address fear/surprise confusion (8% confusion rate)
        - Better handling of mixed emotions
        
        **Deployment Optimization**:
        - Model quantization (FP32 ‚Üí INT8)
        - Knowledge distillation to smaller student model
        - ONNX conversion for cross-platform deployment
        """)

st.markdown("---")
st.caption("Machine Learning & Deep Learning Implementation | Model training and evaluation results")