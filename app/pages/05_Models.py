import streamlit as st
import sys
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent  # points to app/
IMG_DIR = BASE_DIR / "assets" / "images" / "models"

sys.path.append(str(BASE_DIR))
from utils.chatbot_ui import load_css

st.set_page_config(page_title="Models", page_icon="ðŸ¤–", layout="wide")
load_css("global.css", "models.css")

st.title("Machine Learning and Deep Learning Implementation")
st.markdown("---")

st.info("""
**Overview**: This section presents the machine learning and deep learning models implemented 
for speech emotion recognition, their architectures, training procedures, and evaluation results.
""")

# Main tabs for different models
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Traditional ML",
    "CNN",
    "CNN Hybrid Model (Spectrogram)",
    "Wav2Vec2",
    "Cross-Dataset Generalization"
])

# ==================== TAB 1: TRADITIONAL ML ====================
with tab1:
    st.header("Baseline Models")
    st.write("""
    Baseline approaches using raw waveform and MFCC features with 1D CNNs.
    """)

    st.markdown("---")

    # Raw Waveform â€“ 1D CNN
    st.subheader("1. Raw Waveform â€“ 1D CNN")
    st.write("""
    Uses the preprocessed audio signal directly in the time domain. All samples were standardized to 4 seconds, and a
    shallow 1D CNN was applied to capture local temporal variations without frequency transformation.
    """)
    st.write("""
    The model achieved low accuracy and highly unstable validation performance (Figure 1). Large input dimensionality
    led to rapid overfitting; amplitude-only signals lack explicit pitch/timbral information.
    """)

    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        st.image(
            str(IMG_DIR / "raw_waveform_accuracy.png"),
            caption="Figure 1: Validation accuracy of the raw waveform model",
            use_container_width=True
        )

    # Bullet list (Markdown) for limitations
    st.error("""
- Does not encode frequency structure; pitch and formant-related emotion cues are lost.
- High dimensionality leads to inefficient learning and long training times.
- Performs poorly on small datasets due to lack of inductive bias.
    """)

    st.markdown("---")

    # MFCC â€“ 1D CNN
    st.subheader("2. MFCC â€“ 1D CNN")
    st.write("""
    MFCCs (40 Ã— 199) were extracted per audio and used to train a lightweight 1D CNN. MFCCs compactly represent the
    spectral envelope and are widely used in speech recognition.
    """)
    st.write("""
    Achieved moderate performance, clearly better than raw waveform, but below spectrogram-based CNNs. Misclassifications
    were common for emotions with overlapping cepstral patterns (happy, surprised, fearful) due to MFCC compression of
    fine-grained harmonic/energy variations.
    """)

    m1, m2, m3 = st.columns([1, 2, 1])
    with m2:
        st.image(
            str(IMG_DIR / "mfcc_1dcnn_accuracy.png"),
            caption="Figure 2: Accuracy of MFCC 1D CNN model",
            use_container_width=True
        )

    # Bullet list (Markdown) for limitations
    st.warning("""
- MFCCs remove detailed spectral and harmonic information through DCT compression.
- Lack sensitivity to pitch contours, affecting recognition of high-arousal emotions.
- Fixed-length normalization may truncate emotionally relevant segments.
    """)

    st.markdown("---")

    # Spectrogram-Based CNN Models
    st.header("Spectrogram-Based CNN Models")
    st.subheader("Fixed-Size Spectrogram CNN")
    st.write("""
    Log-Mel spectrograms (64 Ã— 128) with standardized padding/truncation ensure consistent inputs. Three conv blocks
    + max-pooling learn hierarchical timeâ€“frequency features; global average pooling + SoftMax produce final predictions.
    """)
    st.info("""
Performance overview:
- Training accuracy: â‰ˆ 94â€“95%
- Test accuracy: â‰ˆ 84â€“86% (Figure 13)
- Mild overfitting observed across heterogeneous datasets.
    """)

    s1, s2, s3 = st.columns([1, 2, 1])
    with s2:
        st.image(
            str(IMG_DIR / "fixed_size_cnn_curve.png"),
            caption="Figure 3: Training vs testing accuracy for spectrogram CNN model",
            use_container_width=True
        )

    st.subheader("Comparison Overview")
    # Pipe table (Markdown)
    st.markdown("""
| Feature type        | Model style | Parameters | Performance | Strengths                                   |
|---------------------|-------------|------------|-------------|---------------------------------------------|
| Raw waveform        | 1D CNN      | High       | Low         | No handcrafted feature required             |
| MFCC                | 1D CNN      | Low        | Moderate    | Efficient, compact representation           |
| Log-Mel spectrogram | 2D CNN      | Moderate   | High        | Preserves spectral structure; best for CNNs |
    """)

    t1, t2, t3 = st.columns([1, 2, 1])
    with t2:
        st.image(
            str(IMG_DIR / "feature_comparison_accuracy.png"),
            caption="Figure 4: Accuracy comparison between waveform, MFCC, and spectrogram CNN",
            use_container_width=True
        )

# ==================== TAB 2: CNN ====================
with tab2:
    st.header("Convolutional Neural Network (CNN)")
    st.subheader("Sliding-Window Spectrogram Approach")
    
    st.write("""
    To enhance temporal feature representation and improve model generalization, the spectrogram-based input was 
    processed using a sliding-window segmentation technique.
    """)
    
    st.markdown("---")
    
    # Approach overview
    st.subheader("Approach")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.write("""
        **Sliding-Window Segmentation**:
        - Window size: 128 Ã— 128 pixels
        - Overlap: 10% along time axis
        - Preserves temporal continuity without distortion
        - Increases effective training samples
        
        **Architecture**:
        - Similar to baseline spectrogram CNN
        - Three convolutional blocks + max-pooling
        - Global average pooling â†’ fully connected layers
        - Softmax output (8 emotions)
        
        **Prediction Strategy**:
        - Individual windows processed separately
        - Final prediction: majority voting or probability averaging
        - Ensures robust clip-level classification
        """)
    
    with col2:
        st.info("""
        **Key Benefits**:
        - Temporal continuity preserved
        - More training samples
        - No spectrogram distortion
        - Robust aggregation strategy
        
        **Window Processing**:
        - Size: 128 Ã— 128
        - Overlap: 10%
        - Aggregation: Voting/Averaging
        """)
    
    st.markdown("---")
    
    # Training Performance
    st.subheader("Training Performance")
    
    st.write("""
    **Accuracy Trends**:
    - Training accuracy: Increased steadily to **0.72**
    - Validation accuracy: Plateaued around **~0.60**
    - Indicates mild overfitting
    
    **Loss Trends**:
    - Training loss: Decreased consistently
    - Validation loss: Exhibited fluctuations
    - Suggests variability in generalization across epochs
    """)
    
    # Figure 15: Training curves (centered, medium size)
    c1, c2, c3 = st.columns([1, 3, 1])
    with c2:
        st.image(
            str(IMG_DIR / "sliding_window_training_curve.png"),
            caption="Figure 5: Training vs validation accuracy and loss",
            use_container_width=True
        )
    
    st.markdown("---")
    
    # Evaluation Results
    st.subheader("Evaluation Results")
    
    # Performance metrics table
    st.write("**Classification Report**:")
    st.markdown("""
| Emotion    | Precision | Recall | F1-Score |
|------------|-----------|--------|----------|
| Neutral    | 0.79      | 0.66   | 0.71     |
| Calm       | 0.67      | 0.80   | 0.73     |
| Happy      | 0.56      | 0.48   | 0.52     |
| Sad        | 0.50      | 0.47   | 0.49     |
| Angry      | 0.55      | 0.55   | 0.55     |
| Fearful    | 0.51      | 0.73   | 0.60     |
| Disgust    | 0.64      | 0.57   | 0.61     |
| Surprised  | 0.82      | 0.77   | 0.80     |
| **Overall Accuracy** | | | **0.59** |
    """)
    
    st.markdown("---")
    
    # Confusion Matrix
    st.subheader("Confusion Matrix Analysis")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        # Figure 16: Confusion matrix (use_container_width within column)
        st.image(
            str(IMG_DIR / "sliding_window_confusion_matrix.png"),
            caption="Figure 6: Confusion matrix of spectrogram overlap",
            use_container_width=True
        )
    
    with col2:
        st.write("""
        **Strong Performance**:
        - Neutral: 0.79 precision
        - Surprised: 0.82 precision
        - Calm: 0.80 recall
        
        **Frequent Misclassifications**:
        - Happy â†” Sad â†” Fearful
        - Overlapping acoustic features
        
        **Insights**:
        - Fearful & Disgust: High recall, moderate precision
        - Low-arousal emotions challenging
        """)
    
    st.markdown("---")
    
    # ROC Curve
    st.subheader("ROC Curve & AUC Scores")
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        # Figure 17: ROC curve
        st.image(
            str(IMG_DIR / "sliding_window_roc_curve.png"),
            caption="Figure 7: ROC curve",
            use_container_width=True
        )
    
    with col2:
        st.success("""
        **High AUC Scores**:
        
        Most classes show strong discriminative capability despite moderate overall accuracy.
        
        **Top Performers**:
        - Surprised
        - Neutral
        - Calm
        
        **Challenging**:
        - Happy
        - Sad
        - Fearful
        """)
    
    st.markdown("---")
    
    # Summary & Future Work
    st.subheader("Summary & Future Improvements")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **Key Achievements**:
        
        - Temporal feature capture: Sliding-window approach preserved time-dependent patterns
        
        - Class separability: Enhanced compared to fixed-size spectrograms
        
        - Strong discriminative capability: High AUC scores for most classes
        
        - Robust aggregation: Majority voting/averaging for clip-level predictions
        """)
    
    with col2:
        st.warning("""
        **Future Improvements**:
        
        - Attention mechanisms: Better temporal weighting
        
        - Class imbalance: Advanced augmentation or focal loss
        
        - Hybrid architectures: CNNâ€“RNN for sequential context modeling
        
        - Feature fusion: Combine with prosodic/acoustic features
        """)
    
    st.markdown("---")
    
    # Performance summary metrics
    st.subheader("Performance Summary")
    
    metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
    
    metric_col1.metric("Overall Accuracy", "59%")
    metric_col2.metric("Training Accuracy", "72%")
    metric_col3.metric("Validation Accuracy", "~60%")
    metric_col4.metric("Best F1-Score", "0.80", "Surprised")
    
    st.caption("""
    **Note**: While overall accuracy (59%) indicates room for improvement, the sliding-window approach 
    successfully enhanced temporal feature representation and achieved strong AUC scores across most emotion classes.
    """)

# ==================== TAB 3: RNN/LSTM ====================
with tab3:
    st.header("Recurrent Neural Networks (RNN/LSTM)")
    
    st.write("""
    Recurrent architectures designed to capture temporal dependencies in sequential audio data.
    Fixed-size CNNs fail to model temporal progression across utterances. Hybrid CNN-RNN models
    address this by combining convolutional feature extraction with recurrent temporal modeling.
    """)
    
    st.markdown("---")
    
    # Sub-tabs for better organization
    subtab1, subtab2, subtab3 = st.tabs([
        "Emotion Recognition (CNN-RNN/LSTM)",
        "Sentiment Analysis (RNN/LSTM)",
        "Sentiment with ResNet"
    ])
    
    # ==================== SUBTAB 1: EMOTION (CNN HYBRID) ====================
    with subtab1:
        st.subheader("CNN Hybrid Models for Emotion Recognition")
        
        st.write("""
        Hybrid architectures combine CNN front-ends (local spectral features) with recurrent layers 
        (temporal evolution) to capture emotion-critical patterns like pitch contours, rhythm, and energy variations.
        """)
        
        st.markdown("---")
        
        # Data representation
        st.write("**Data Representation**:")
        st.markdown("""
        - Full-frequency spectrograms (0â€“11 kHz) using 256-Mel filter bank
        - Adaptive padding/truncation based on 95th percentile of training lengths
        - Split: 70% train, 15% validation, 15% test
        - Classes: 8 emotions
        """)
        
        st.markdown("---")
        
        # Architectures side-by-side
        st.subheader("Model Architectures")
        
        arch_col1, arch_col2 = st.columns(2)
        
        with arch_col1:
            st.write("**CNN-RNN**:")
            st.code("""
            Spectrogram (256-Mel Ã— Time)
                â†“
        Conv2D Block 1
                â†“
        Conv2D Block 2
                â†“
        Conv2D Block 3
                â†“
        Bidirectional RNN
                â†“
        Global Pooling
                â†“
    Dense + Softmax (8 emotions)
            """, language="text")
            st.info("""
            **Advantages**:
            - Faster training
            - Fewer parameters
            - Better for deployment
            - Competitive performance
            """)
        
        with arch_col2:
            st.write("**CNN-LSTM**:")
            st.code("""
            Spectrogram (256-Mel Ã— Time)
                â†“
        Conv2D Block 1
                â†“
        Conv2D Block 2
                â†“
        Conv2D Block 3
                â†“
        Bidirectional LSTM
                â†“
        Global Pooling
                â†“
    Dense + Softmax (8 emotions)
            """, language="text")
            st.info("""
            **Advantages**:
            - Slightly higher accuracy
            - Better long-range dependencies
            
            **Trade-offs**:
            - Heavier model
            - Slower training
            """)
        
        st.write("""
        **Training Configuration**: Categorical cross-entropy, Adam optimizer, batch size 32, 
        early stopping over 100 epochs.
        """)
        
        st.markdown("---")
        
        # Results: CNN-GRU
        st.subheader("CNN-RNN Results")
        
        res_col1, res_col2 = st.columns([1, 1])
        
        with res_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_classification_report.png"),
                caption="Classification report of CNN-RNN",
                use_container_width=True
            )
        
        with res_col2:
            st.image(
                str(IMG_DIR / "cnn_rnn_per_class_accuracy.png"),
                caption="Per-class accuracy of CNN-RNN",
                use_container_width=True
            )
        
        st.markdown("---")
        
        # Results: CNN-LSTM
        st.subheader("CNN-LSTM Results")
        
        lstm_col1, lstm_col2 = st.columns([1, 1])
        
        with lstm_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_classification_report.png"),
                caption="Figure 20: Classification report of CNN-LSTM",
                use_container_width=True
            )
        
        with lstm_col2:
            st.image(
                str(IMG_DIR / "cnn_lstm_per_class_accuracy.png"),
                caption="Figure 21: Per-class accuracy of CNN-LSTM",
                use_container_width=True
            )
        
        st.markdown("---")
        
        # Confusion Matrix Comparison
        st.subheader("Confusion Matrix Comparison")
        
        st.image(
                str(IMG_DIR / "cnn_hybrid_confusion_matrix.png"),
                caption="Figure 22: Confusion matrix of CNN hybrid models",
                use_container_width=True
        )
        
        st.markdown("---")
        
        # Summary
        st.subheader("Summary")
        
        sum_col1, sum_col2 = st.columns(2)
        
        with sum_col1:
            st.success("""
            **Key Achievements**:
            
            - Significantly outperformed fixed CNN baselines
            - Captured temporal cues: pitch contours, rhythm, energy variations
            - CNN-GRU: Faster, lighter, deployment-friendly
            - CNN-LSTM: Slightly higher accuracy, better long-range modeling
            """)
        
        with sum_col2:
            st.info("""
            **Future Improvements**:
            
            - Attention mechanisms for temporal weighting
            - Deeper recurrent layers
            - Transformer embeddings (e.g., Wav2Vec2)
            - Hybrid CNN-Transformer architectures
            """)
    
    # ==================== SUBTAB 2: SENTIMENT (RNN/LSTM) ====================
    with subtab2:
        st.subheader("Sentiment Analysis with RNN/LSTM")
        
        st.write("""
        To complement eight-class emotion classification, a secondary experiment grouped emotions into three broader 
        sentiment categories. This approach reduces label noise across heterogeneous datasets and enables evaluation 
        on coarser affective dimensions.
        """)
        
        st.markdown("---")
        st.subheader("Data Preparation & Sentiment Grouping")
    
        st.write("**Sentiment Categories**:")

        sent_group_col1, sent_group_col2, sent_group_col3 = st.columns(3)

        with sent_group_col1:
            st.success("""
            **Positive**:
            - Happy
            - Surprised
            """)

        with sent_group_col2:
            st.info("""
            **Moderate**:
            - Neutral
            - Calm
            """)

        with sent_group_col3:
            st.error("""
            **Negative**:
            - Sad       - Fearful
            - Angry     - Disgust
            """)

        st.markdown("---")
        # Dataset Preparation
        st.write("**Dataset Preparation**:")
        st.write("""
        Four datasets (RAVDESS, TESS, SAVEE, CREMA-D) were merged and balanced using stratified down-sampling.
        Fine-grained emotion metadata was preserved for misclassification analysis.
        """)

        # Distribution comparison
        dist_col1, dist_col2 = st.columns(2)

        with dist_col1:
            st.image(
                str(IMG_DIR / "sentiment_distribution_imbalanced.png"),
                caption="Distribution of sentiment classes (imbalanced)",
                use_container_width=True
            )

        with dist_col2:
            st.image(
                str(IMG_DIR / "sentiment_distribution_balanced.png"),
                caption="Distribution of sentiment classes (balanced)",
                use_container_width=True
            )

            st.markdown("---")
            # Models Overview
        st.subheader("Models Developed")

        model_col1, model_col2, model_col3 = st.columns(3)

        with model_col1:
            st.info("""
            **CNN-RNN**:
            - Time-distributed CNN front-end
            - Bidirectional RNN
            - Sequential modeling
            """)

        with model_col2:
            st.info("""
            **CNN-LSTM**:
            - Similar CNN extractor
            - Bidirectional LSTM
            - Long-range dependencies
            """)

        with model_col3:
            st.info("""
            **CNN-ResNet**:
            - Residual CNN
            - RGB spectrograms (magma colormap)
            - High-capacity baseline
            """)

        st.write("""
        **Training Configuration**: Categorical cross-entropy, Adam optimizer, early stopping, model checkpointing.
        """)

        st.markdown("---")

        # ========== CNN-RNN RESULTS ==========
        st.header("CNN-RNN Results")

        st.write("""
        The CNN-RNN architecture achieved an **overall accuracy of 83%** on the test set, demonstrating strong 
        performance in sentiment-level classification. Precision and recall values were balanced across classes.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_training_curve.png"),
                caption="Accuracy and loss (CNN-RNN)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Stable accuracy progression
            - Decreasing training loss
            - Validation loss fluctuations suggest mild overfitting
            - Good generalization overall
            """)

            st.metric("Test Accuracy", "83%")
            st.metric("Training Time", "~45 min")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([1, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_classification_report.png"),
                caption="Figure 26: Classification report of sentiment CNN-RNN",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Moderate Sentiment**:
            - Highest recall: 0.90

            **Positive Sentiment**:
            - Highest precision: 0.85

            **Balanced Performance**:
            - Precision and recall well-distributed
            - Strong classification across all sentiments
            """)

        st.markdown("---")

        # Confusion Matrix & ROC
        st.subheader("Confusion Matrix & ROC Analysis")
        im_col1, im_col2 = st.columns([1, 1])
        with im_col1:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-RNN",
                use_container_width=True
            )
        with im_col2:
            st.image(
                str(IMG_DIR / "cnn_rnn_sentiment_roc.png"),
                caption="ROC of sentiment CNN-RNN",
                use_container_width=True
            )
        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.write("""
            **Confusion Matrix Insights**:
            - Most errors between Negative â†” Positive sentiments
            - Reflects acoustic similarities in emotional expressions
            - Moderate sentiment well-separated from extremes
            - Overall strong diagonal (correct predictions)
            """)

        with analysis_col2:
            st.write("""
            **ROC Analysis**:
            - Excellent separability confirmed
            - AUC scores **> 0.90** for all classes
            - Strong discriminative capability
            - Robust performance across sentiment categories
            """)

        st.markdown("---")

        # Summary
        st.subheader("CNN-RNN Summary")

        st.info("""
        **Key Achievements**:

         **83% test accuracy** - Strong sentiment classification  
         **Balanced precision/recall** - No significant class bias  
         **High AUC (>0.90)** - Excellent class separability  
         **Computational efficiency** - Faster than CNN-LSTM  
         **Stable training** - Consistent convergence  

        **Trade-offs**:

        - Mild overfitting observed in validation loss  
        - Negative â†” Positive confusion due to acoustic overlap  

        **Conclusion**:
        The CNN-RNN model provides an **effective trade-off between accuracy and computational efficiency** 
        for sentiment grouping tasks, making it suitable for real-time applications.
        """)

        st.markdown("---")

        # ========== CNN-LSTM SECTION==========
        st.header("CNN-LSTM Results")

        st.write("""
        The CNN-LSTM architecture achieved an **overall accuracy of 81%**, slightly lower than CNN-RNN but with 
        improved recall for certain sentiment classes. The model demonstrates robust temporal modeling capabilities 
        through its bidirectional LSTM layers.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_training_curve.png"),
                caption="Figure 28: Accuracy and loss (CNN-LSTM)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Excellent convergence for training accuracy and loss
            - Rising validation loss after epoch 10 suggests overfitting
            - Longer training time than CNN-RNN
            - Higher model complexity
            """)

            st.metric("Test Accuracy", "81%")
            st.metric("Training Time", "~60 min")
            st.caption("33% slower than CNN-RNN")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([2, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_classification_report.png"),
                caption="Figure 29: Classification report of sentiment CNN-LSTM",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Moderate Sentiment**:
            - Highest recall: 0.89
            - Captures neutral tones effectively

            **Positive Sentiment**:
            - Improved recall: 0.83
            - Better than CNN-RNN (0.80)

            **Strength**:
            - Better long-range dependency modeling
            - Enhanced temporal context capture
            """)

        st.markdown("---")

        # Confusion Matrix & ROC
        st.subheader("Confusion Matrix & ROC Analysis")

        conf_roc_col1, conf_roc_col2 = st.columns([1, 1])

        with conf_roc_col1:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-LSTM",
                use_container_width=True
            )

        with conf_roc_col2:
            st.image(
                str(IMG_DIR / "cnn_lstm_sentiment_roc.png"),
                caption="ROC of sentiment CNN-LSTM",
                use_container_width=True
            )

        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.write("""
            **Confusion Matrix Insights**:
            - Most errors between Negative â†” Positive sentiments
            - Consistent with acoustic similarities in emotional speech
            - Moderate sentiment well-separated
            - Overall strong diagonal performance
            """)

        with analysis_col2:
            st.write("""
            **ROC Analysis**:
            - Strong separability confirmed
            - AUC values **> 0.90** for all classes
            - Excellent discriminative capability
            - Robust performance across sentiments
            """)

        st.markdown("---")

        # Misclassification Analysis
        st.subheader("Misclassification Analysis")

        st.write("""
        Pie charts illustrate error patterns for cross-sentiment misclassifications, revealing which emotions 
        are most commonly confused when predictions are incorrect.
        """)

        st.image(
            str(IMG_DIR / "cnn_lstm_misclassification_analysis.png"),
            caption="Figure 31: Misclassification Analysis for CNN-LSTM",
            use_container_width=True
        )

        error_col1, error_col2 = st.columns(2)

        with error_col1:
            st.error("""
            **Negative â†’ Positive Errors**:

            When the model incorrectly predicts Positive for Negative samples:

            - **Fearful**: 34% (high arousal overlap)
            - **Angry**: 32% (intensity similarity)
            - **Sad**: 20%
            - **Disgust**: 14%

            **Cause**: High-arousal negative emotions (fearful, angry) share 
            acoustic features with positive emotions (energy, pitch variation).
            """)

        with error_col2:
            st.warning("""
            **Positive â†’ Negative Errors**:

            When the model incorrectly predicts Negative for Positive samples:

            - **Happy**: 87% (dominant error source)
            - **Surprised**: 13%

            **Cause**: Happy samples with certain prosodic patterns 
            (lower energy, subdued expression) can be misinterpreted 
            as negative sentiments.
            """)

        st.info("""
        **Key Insight**: 
        The majority of cross-sentiment errors involve **high-arousal emotions** (fearful, angry, happy) 
        due to shared acoustic characteristics like pitch variation and energy levels. Moderate sentiment 
        errors are minimal due to distinct low-arousal prosody.
        """)

        st.markdown("---")

        # Summary
        st.subheader("CNN-LSTM Summary")

        sum_col1, sum_col2 = st.columns(2)

        with sum_col1:
            st.success("""
            **Strengths**:

            **81% test accuracy** - Strong performance  
            **Improved Positive recall** (0.83 vs 0.80 in CNN-RNN)  
            **Best Moderate recall** (0.89)  
            **High AUC (>0.90)** - Excellent separability  
            **Better temporal modeling** - Long-range dependencies  
            """)

        with sum_col2:
            st.warning("""
            **Trade-offs**:

            **Slightly lower accuracy** than CNN-RNN (81% vs 83%)  
            **Overfitting** - Validation loss rises after epoch 10  
            **Longer training** - ~60 min vs ~45 min (CNN-RNN)  
            **Higher complexity** - More parameters (~3M vs ~2M)  
            **Negative â†” Positive confusion** - Acoustic overlap issues  
            """)

        st.info("""
        **Conclusion**:

        CNN-LSTM provides **robust temporal modeling** with improved recall for Positive sentiment and excellent 
        Moderate sentiment detection. However, it comes at the cost of:

        - **Longer training time** (33% slower than CNN-RNN)
        - **Higher computational complexity**
        - **Mild overfitting** after epoch 10

        **Recommendation**:
        - Use CNN-LSTM when **maximum recall** is critical (especially for Positive/Moderate)
        - Use CNN-RNN when **faster training** and **better accuracy** are priorities
        - For production: CNN-RNN offers better **efficiency-accuracy balance** (83% accuracy, faster training)
        """)

        st.markdown("---")
    
    # ==================== SUBTAB 3: SENTIMENT WITH RESNET ====================
    with subtab3:
        st.subheader("Sentiment Analysis with CNN-ResNet")

        st.write("""
        Combining ResNet's deep residual architecture with convolutional feature extraction for sentiment classification.
        ResNet's skip connections enable training of very deep networks while maintaining gradient flow.
        """)

        st.markdown("---")

        # Architecture
        st.subheader("Architecture Overview")

        arch_col1, arch_col2 = st.columns([3, 2])

        with arch_col1:
            st.write("**CNN-ResNet Architecture**:")
            st.code("""
RGB Spectrogram Input (Magma Colormap)
        â†“
Initial Conv2D (64 filters, 7Ã—7, stride 2)
        â†“
Max Pooling (3Ã—3, stride 2)
        â†“
Residual Block 1 (64 filters)
  â”œâ”€ Conv2D (64, 3Ã—3)
  â”œâ”€ BatchNorm + ReLU
  â”œâ”€ Conv2D (64, 3Ã—3)
  â””â”€ Skip Connection (+)
        â†“
Residual Block 2 (128 filters, stride 2)
  â”œâ”€ Conv2D (128, 3Ã—3)
  â”œâ”€ BatchNorm + ReLU
  â”œâ”€ Conv2D (128, 3Ã—3)
  â””â”€ Skip Connection (1Ã—1 conv projection)
        â†“
Residual Block 3 (256 filters, stride 2)
        â†“
Global Average Pooling
        â†“
Dense (128) + ReLU + Dropout(0.5)
        â†“
Dense (3 sentiments) + Softmax
        â†“
Output: Sentiment Probabilities
            """, language="text")

        with arch_col2:
            st.info("""
            **Key Features**:

            **RGB Spectrograms**:
            - Magma colormap
            - 3-channel input
            - Enhanced visual patterns

            **Residual Connections**:
            - Skip connections
            - Gradient flow preservation
            - Deeper networks possible

            **Architecture**:
            - 3 residual blocks
            - Increasing filter depth
            - Global average pooling

            **Parameters**: ~2.5M
            **Input**: 224Ã—224Ã—3 RGB spectrograms
            """)

        st.markdown("---")

        # Results Overview
        st.header("CNN-ResNet Results")

        st.write("""
        The CNN-ResNet model achieved **76.15% accuracy**, lower than CNN-RNN (83%) and CNN-LSTM (81%), but 
        excelled in precision for Positive sentiment and recall for Moderate sentiment. The deep residual 
        architecture provides superior feature extraction capabilities.
        """)

        # Training Performance
        st.subheader("Training Performance")

        train_col1, train_col2 = st.columns([3, 2])

        with train_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_training_curve.png"),
                caption="Accuracy and loss (CNN-ResNet)",
                use_container_width=True
            )

        with train_col2:
            st.write("""
            **Training Observations**:
            - Steady convergence demonstrated
            - Validation loss fluctuations indicate overfitting risks
            - Slower convergence than CNN-RNN
            - Deep architecture requires more epochs
            """)

            st.metric("Test Accuracy", "76.15%")
            st.metric("Training Time", "~75 min")
            st.caption("Slower due to deeper architecture")

        st.markdown("---")

        # Classification Report
        st.subheader("Classification Performance")

        class_col1, class_col2 = st.columns([2, 1])

        with class_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_classification_report.png"),
                caption="Classification report of sentiment CNN-ResNet",
                use_container_width=True
            )

        with class_col2:
            st.success("""
            **Best Performers**:

            **Positive Sentiment**:
            - Highest precision: 0.82
            - Strong positive detection

            **Moderate Sentiment**:
            - Highest recall: 0.83
            - Excellent neutral tone capture

            **Strength**:
            - Superior feature extraction for Negative and Moderate
            - Deep residual learning captures complex patterns
            """)

        st.markdown("---")

        # Confusion Matrix
        st.subheader("Confusion Matrix Analysis")

        conf_col1, conf_col2 = st.columns([3, 2])

        with conf_col1:
            st.image(
                str(IMG_DIR / "cnn_resnet_sentiment_confusion.png"),
                caption="Confusion matrix of sentiment CNN-ResNet",
                use_container_width=True
            )

        with conf_col2:
            st.write("""
            **Confusion Patterns**:

            - Most errors: Negative â†” Positive
            - Consistent with CNN-RNN and CNN-LSTM
            - Acoustic similarity in high-arousal emotions
            - Moderate sentiment well-separated

            **Performance**:
            - Strong diagonal (correct predictions)
            - Balanced misclassification distribution
            - No severe class bias
            """)

        st.markdown("---")

        # Detailed Analysis
        st.subheader("Detailed Analysis")

        analysis_col1, analysis_col2, analysis_col3 = st.columns(3)

        with analysis_col1:
            st.metric("Positive Precision", "0.82", "Best")
            st.caption("Superior positive sentiment detection")

        with analysis_col2:
            st.metric("Moderate Recall", "0.83", "Best")
            st.caption("Excellent neutral tone capture")

        with analysis_col3:
            st.metric("Overall Accuracy", "76.15%")
            st.caption("Lower than RNN variants")

        st.markdown("---")

        # Strengths vs Trade-offs
        st.subheader("CNN-ResNet Summary")

        sum_col1, sum_col2 = st.columns(2)

        with sum_col1:
            st.success("""
            **Strengths**:

            **Best Positive precision** (0.82) - Superior positive detection  
            **Best Moderate recall** (0.83) - Excellent neutral capture  
            **Deep feature extraction** - Residual learning captures complex patterns  
            **RGB spectrograms** - Enhanced visual representation  
            **Superior Negative/Moderate features** - Strong class-specific extraction  
            **Residual connections** - Better gradient flow than vanilla CNNs  
            """)

        with sum_col2:
            st.warning("""
            **Trade-offs**:

            **Lower overall accuracy** - 76.15% vs 83% (CNN-RNN), 81% (CNN-LSTM)  
            **Overfitting risks** - Validation loss fluctuations observed  
            **Slower training** - ~75 min vs ~45 min (CNN-RNN)  
            **More parameters** - ~2.5M (higher complexity)  
            **Resource intensive** - Requires more memory and compute  
            **Negative â†” Positive confusion** - Consistent with other models  
            """)

        st.markdown("---")

        # When to Use
        st.subheader("When to Use CNN-ResNet")

        st.info("""
        **Best Use Cases**:

        **Resource-Rich Environments**: When GPU/memory constraints are not an issue  
        **Positive Sentiment Focus**: Applications requiring high precision for positive detection  
        **Moderate Sentiment Emphasis**: When neutral tone detection is critical  
        **Deep Feature Learning**: Complex datasets requiring hierarchical feature extraction  
        **RGB Spectrogram Advantage**: When colormap information provides added value  
        **Not Recommended For**:

        **Real-time applications** - Slower than CNN-RNN/LSTM  
        **Resource-constrained devices** - Higher memory footprint  
        **Maximum overall accuracy** - CNN-RNN performs better (83%)  
        **Rapid prototyping** - Longer training time  
        """)

        st.markdown("---")

        # Comparison with Other Models
        st.subheader("Model Comparison")

        st.markdown("""
        | Metric | CNN-RNN | CNN-LSTM | CNN-ResNet |
        |--------|---------|----------|------------|
        | **Overall Accuracy** | 83% | 81% | 76.15% |
        | **Positive Precision** | 0.85 | 0.80 | 0.82 |
        | **Moderate Recall** | 0.90 | 0.89 | 0.83 |
        | **Training Time** | ~45 min | ~60 min | ~75 min |
        | **Parameters** | ~2M | ~3M | ~2.5M |
        | **Memory Usage** | Low | Medium | High |
        | **Feature Extraction** | Good | Better | Best |
        | **Gradient Flow** | Good | Better | Best |

        **Verdict**:
        - **Production/Real-time**: CNN-RNN (best accuracy + efficiency)
        - **Maximum Recall**: CNN-RNN (0.90 Moderate recall)
        - **Deep Learning Research**: CNN-ResNet (best feature extraction)
        - **Balanced Performance**: CNN-LSTM (middle ground)
        """)

        st.markdown("---")

        # Technical Insights
        with st.expander("Technical Insights & Analysis"):
            st.write("""
            ### Why CNN-ResNet Has Lower Accuracy Despite Better Features?

            **1. Over-Parameterization**:
            - 2.5M parameters may be excessive for 3-class sentiment task
            - Leads to overfitting on limited data
            - Validation loss fluctuations confirm this

            **2. RGB Spectrogram Trade-off**:
            - Adds visual complexity but may introduce noise
            - Magma colormap not optimized for acoustic patterns
            - Single-channel mel-spectrograms more effective for RNN variants

            **3. Architecture Mismatch**:
            - ResNet designed for image classification (ImageNet)
            - Audio spectrograms have different statistical properties
            - Skip connections may not provide same benefit as in vision tasks

            **4. Training Dynamics**:
            - Deeper networks require more careful hyperparameter tuning
            - Learning rate, batch size, regularization need optimization
            - Current setup favors shallower RNN architectures

            ### What CNN-ResNet Does Better:

            **Hierarchical Features**:
            - Residual blocks learn multi-scale patterns
            - Better spectral-temporal feature composition
            - Superior for fine-grained sentiment nuances

            **Gradient Flow**:
            - Skip connections prevent vanishing gradients
            - Enables training of deeper networks
            - Better feature backpropagation

            **Class-Specific Strengths**:
            - Positive: 0.82 precision (detects true positives accurately)
            - Moderate: 0.83 recall (captures neutral sentiments well)
            - These specific strengths valuable for targeted applications

            ### Potential Improvements:

            1. **Reduce Complexity**: Use ResNet-18 instead of deeper variants
            2. **Regularization**: Stronger dropout, L2 penalty, early stopping
            3. **Data Augmentation**: SpecAugment, mixup for better generalization
            4. **Ensemble**: Combine with CNN-RNN for complementary strengths
            5. **Transfer Learning**: Pre-train on larger audio dataset first
            6. **Single-Channel Input**: Test with grayscale spectrograms
            """)

        st.markdown("---")

        # Final Conclusion
        st.success("""
        **Conclusion**:

        CNN-ResNet demonstrates **superior feature extraction capabilities** with best-in-class Positive precision (0.82) 
        and Moderate recall (0.83). However, the deeper architecture comes with trade-offs:

        - **7% lower accuracy** than CNN-RNN (76.15% vs 83%)
        - **67% longer training time** (~75 min vs ~45 min)
        - **Overfitting tendencies** due to higher complexity

        **Suitable for**:
        - Resource-rich deployment environments
        - Applications prioritizing Positive/Moderate sentiment detection
        - Research exploring deep residual learning for audio

        **For production sentiment analysis**, **CNN-RNN remains the recommended choice** due to its superior 
        balance of accuracy (83%), efficiency (~45 min training), and generalization performance.
        """)

# ==================== TAB 4: WAV2VEC2 ====================
with tab4:
    st.header("Wav2Vec2 - Transformer-Based Speech Emotion Recognition")
    
    st.write("""
    State-of-the-art transformer-based model pre-trained on unlabeled speech, 
    fine-tuned for multi-class emotion and sentiment classification.
    """)
    
    st.markdown("---")
    
    # Sub-tabs for better organization
    wav2vec_subtab1, wav2vec_subtab2, wav2vec_subtab3 = st.tabs([
        "Multi-Class Emotion (8 Classes)",
        "Sentiment Classification (3 Classes)",
        "Hierarchical Negative-Emotion Recognition"
    ])
    
    # ==================== SUBTAB 1: 8-CLASS EMOTION ====================
    with wav2vec_subtab1:
        st.subheader("Fine-Tuned Model for Multi-Class Emotion Recognition")

        st.write("""
        A Wav2Vec2-Base model was fine-tuned for 8-emotion classification (angry, happy, sad, neutral, 
        fearful, disgust, surprised, calm) on the unified, pre-processed audio dataset.
        """)

        st.markdown("---")

        # Training Configuration
        st.write("**Training Configuration**:")

        train_config_col1, train_config_col2 = st.columns(2)

        with train_config_col1:
            st.markdown("""
            **Model & Data**:
            - Base Model: Wav2Vec2-Base (95M parameters)
            - Pre-trained on: 960h LibriSpeech
            - Classes: 8 emotions
            - Data Split: 80% train, 10% validation, 10% test
            """)

        with train_config_col2:
            st.markdown("""
            **Training Details**:
            - Optimizer: Adam (lr = 3e-5)
            - Batch Size: 32
            - Precision: FP16 mixed
            - Epochs: 50
            - Audio Resampling: 16 kHz
            """)

        st.markdown("---")

        # Results Overview
        st.subheader("Performance Results (8 Emotions)")

        st.write("""
        The Wav2Vec2 model achieved **76% overall accuracy** on the test set, 
        outperforming traditional CNN-based baselines in feature representation.
        """)

        # Classification Report
        result_col1, result_col2 = st.columns([2, 1])

        with result_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_8emotion_classification_report.png"),
                caption="Figure 35: Classification report of Wav2Vec2 (8 emotions)",
                use_container_width=True
            )

        with result_col2:
            st.success("""
            **Best Performers**:

            **Calm**: F1 = 0.90
            - Strong low-arousal detection

            **Angry**: F1 = 0.82
            - Good high-energy emotion recognition

            **Challenging**:
            - Happy & Surprise
            - Limited samples
            - Acoustic similarity
            """)

        st.markdown("---")

        # Confusion Matrix
        st.subheader("Confusion Matrix Analysis")

        conf_col1, conf_col2 = st.columns([2, 1])

        with conf_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_8emotion_confusion_matrix.png"),
                caption="Figure 36: Confusion matrix Wav2Vec2 (8 emotions)",
                use_container_width=True
            )

        with conf_col2:
            st.write("""
            **Key Observations**:

            **Strong Performance**:
            - Robust for high-energy emotions
            - Clear class separation for angry/calm

            **Common Confusions**:
            - Neutral â†” Surprise
            - Happy â†” Surprise
            - Acoustic overlap issues

            **Insight**: Arousal-based separation works well
            """)

        st.markdown("---")

        # Training Curves
        st.subheader("Training Dynamics")

        st.image(
            str(IMG_DIR / "wav2vec2_8emotion_training_curve.png"),
            caption="Figure 37: Validation accuracy over epochs for Wav2Vec2",
            use_container_width=True
        )

        train_insight_col1, train_insight_col2 = st.columns(2)

        with train_insight_col1:
            st.write("""
            **Convergence Behavior**:
            - Rapid convergence (stable by epoch 15)
            - Strong initial generalization
            - Rising validation loss after peak
            """)

        with train_insight_col2:
            st.warning("""
            **Overfitting Indicators**:
            - Validation loss increases after convergence
            - Gap between train and validation accuracy
            - Suggests need for regularization
            """)

        st.markdown("---")

        # ========== SENTIMENT MAPPING (POST-PROCESSING) ==========
        st.header("Testing with 3-Sentiment Mapping")

        st.write("""
        To assess the robustness of the fine-tuned Wav2Vec2 model, its **eight emotion predictions 
        were mapped into three sentiment categories** without retraining. Instead, raw predictions were 
        post-processed using a mapping strategy, aggregating per-emotion probabilities into sentiment-level scores.
        """)

        st.markdown("---")

        # Sentiment Mapping
        st.subheader("Sentiment Grouping Strategy")

        st.write("**Emotion-to-Sentiment Mapping**:")

        mapping_col1, mapping_col2, mapping_col3 = st.columns(3)

        with mapping_col1:
            st.success("""
            **Positive Sentiment**:
            - Happy
            - Surprised
            """)

        with mapping_col2:
            st.info("""
            **Moderate Sentiment**:
            - Neutral
            - Calm
            """)

        with mapping_col3:
            st.error("""
            **Negative Sentiment**:
            - Angry
            - Sad
            - Fearful
            - Disgust
            """)

        st.write("""
        **Post-Processing Method**:
        - Extract per-emotion probabilities from 8-class predictions
        - Aggregate probabilities within each sentiment group
        - Assign final sentiment based on highest aggregated probability
        - No model retraining required (purely inference-level mapping)
        """)

        st.markdown("---")

        # Results
        st.subheader("Sentiment-Level (mapping emotion to sentiment) Testing Results")

        st.write("""
        Evaluation using a speaker-independent test set produced a classification report and a 3Ã—3 confusion matrix. 
        Results showed that **sentiment-level prediction was more stable** than fine-grained emotion classification, 
        reducing ambiguity and improving reliability.
        """)

        # Classification Report
        sent_result_col1, sent_result_col2 = st.columns([2, 1])

        with sent_result_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_sentiment_mapping_classification_report.png"),
                caption="Classification report of Wav2Vec2 with sentiment mapping",
                use_container_width=True
            )

        with sent_result_col2:
            st.success("""
            **Performance Improvement**:

            **Better than 8-class**:
            - More stable predictions
            - Higher per-class accuracy
            - Reduced confusion

            **Best Performers**:
            - Positive & Negative: High accuracy
            - Moderate: Slightly lower due to acoustic similarity

            **Key Insight**: 
            Coarser granularity improves stability!
            """)

        st.markdown("---")

        # Confusion Matrix
        st.subheader("Confusion Matrix Analysis (3-Sentiment)")

        conf_sent_col1, conf_sent_col2 = st.columns([2, 1])

        with conf_sent_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_sentiment_mapping_confusion_matrix.png"),
                caption="Figure 38: Confusion matrix for Wav2Vec2 sentiment-level",
                use_container_width=True
            )

        with conf_sent_col2:
            st.write("""
            **Key Observations**:

            **Strong Diagonal**:
            - Positive: High reliability
            - Negative: High reliability

            **Moderate Challenges**:
            - Slight overlap with Negative
            - Due to acoustic similarity (calm/neutral vs sad)
            - Minor confusion acceptable for coarse classification

            **Verdict**: Excellent generalization
            """)

        st.markdown("---")

        # Detailed Analysis
        st.subheader("Detailed Analysis")

        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.write("""
            **Why Sentiment Mapping Works**:

            1. **Reduced Complexity**: 8 classes â†’ 3 classes
            2. **Better Separation**: Sentiment groups are more distinct
            3. **Acoustic Alignment**: Emotions within sentiment share characteristics
            4. **Error Aggregation**: Grouping masks fine-grained confusions

            **Positive & Negative**:
            - High arousal distinction clear
            - Well-separated acoustic features
            - Excellent recognition accuracy

            **Moderate Challenges**:
            - Neutral/Calm similar to Sad
            - Low arousal overlap issues
            - Minor confusions expected
            """)

        with analysis_col2:
            st.info("""
            **Comparison: 8-Emotion vs Sentiment Mapping**:

            **8-Emotion Performance**:
            - Overall Accuracy: 76%
            - Best F1: 0.90 (Calm)
            - Challenges: Happy, Surprise

            **3-Sentiment Performance**:
            - Overall Accuracy: ~88-92%* (*estimated)
            - Best F1: ~0.95+ (Positive/Negative)
            - Challenges: Moderate distinction

            **Key Finding**:
            Moving from 8 to 3 classes improves stability 
            significantly without retraining!
            """)

        st.markdown("---")

        # Key Finding
        st.subheader("Key Finding: No Retraining Needed")

        st.success("""
        **Zero-Shot Sentiment Classification**:

        Post-processing 8-emotion predictions into 3-sentiment categories achieves excellent results 
        **without any retraining**. This demonstrates:

        **Model Robustness**: Pre-trained patterns support coarser classification  
        **Efficient Deployment**: Single model serves multiple granularities  
        **Flexibility**: Easy to add/modify sentiment mappings  
        **Cost Savings**: No additional training computational cost  

        **Practical Implications**:
        - Deploy single 8-emotion model
        - Map outputs to sentiments as needed
        - Maintain option for fine-grained emotion analysis
        - Better user experience through clearer sentiments
        """)

        st.markdown("---")

        # Summary
        st.subheader("Summary & Conclusions")

        sum_col1, sum_col2 = st.columns(2)

        with sum_col1:
            st.success("""
            **8-Emotion Model Strengths**:

            76% overall accuracy
            Strong calm detection (F1=0.90)
            Good high-energy emotions
            Fine-grained emotion capability

            **Use Cases**:
            - Clinical emotion analysis
            - Research applications
            - Detailed affect recognition
            """)

        with sum_col2:
            st.info("""
            **3-Sentiment Mapping Benefits**:

            ~88-92% estimated accuracy*
            Better stability and clarity
            No retraining required
            Practical deployment advantage

            **Use Cases**:
            - Customer service (positive/negative)
            - User experience assessment
            - Real-time sentiment detection
            - General affect monitoring
            """)

        st.markdown("---")

        # Recommendation
        st.info("""
        ## Deployment Recommendation

        **Deploy the 8-emotion Wav2Vec2 model** and use dynamic sentiment mapping:

        1. **Default**: Show sentiment (3-class) for end-users
        2. **Optional**: Provide detailed emotions (8-class) for analysts
        3. **Flexible**: Support custom sentiment mappings per application
        4. **Efficient**: Single model, multiple outputs

        This approach combines the best of both worlds:
        - Detailed emotion analysis capability (research/clinical)
        - Clear sentiment classification (practical applications)
        - Zero additional training cost
        - Maximum flexibility for different use cases
        """)

    # ==================== SUBTAB 2: 3-CLASS SENTIMENT (TRAINED) ====================
    with wav2vec_subtab2:
        st.subheader("Custom Wav2Vec2 Sentiment Classifier (3 Classes)")
        
        st.write("""
        A custom Wav2Vec2 sentiment model was trained by **freezing all encoder layers** 
        and learning only a **lightweight classification head** (768 â†’ 3). This approach 
        enabled fast adaptation and minimized overfitting.
        """)
        
        st.markdown("---")
        
        # Architecture
        st.write("**Architecture**:")
        
        arch_col1, arch_col2 = st.columns([2, 1])
        
        with arch_col1:
            st.code("""
Raw Waveform (16 kHz)
    â†“
Wav2Vec2-Base Encoder (FROZEN)
    â”œâ”€ Feature Extractor (CNN)
    â”œâ”€ Transformer Encoder (12 layers)
    â””â”€ Hidden States (768-dim)
    â†“
Temporal Pooling (Mean Aggregation)
    â†“
Classification Head (TRAINABLE)
    â”œâ”€ Linear (768 â†’ 256)
    â”œâ”€ ReLU + Dropout(0.3)
    â”œâ”€ Linear (256 â†’ 64)
    â”œâ”€ ReLU + Dropout(0.2)
    â””â”€ Linear (64 â†’ 3)
    â†“
Softmax
    â†“
Output: Sentiment Probabilities
    (Negative / Neutral / Positive)
            """, language="text")
        
        with arch_col2:
            st.info("""
            **Design Benefits**:
            
            **Frozen Encoder**:
            - Prevents catastrophic forgetting
            - Fast training
            - Stable learning
            
            **Lightweight Head**:
            - Minimal parameters
            - Reduced overfitting
            - ~10K trainable params
            """)
        
        st.markdown("---")
        
        # Sentiment Categories
        st.write("**Sentiment Grouping**:")
        
        sent_col1, sent_col2, sent_col3 = st.columns(3)
        
        with sent_col1:
            st.success("""
            **Positive**:
            - Happy
            - Surprised
            """)
        
        with sent_col2:
            st.info("""
            **Neutral**:
            - Neutral
            - Calm
            """)
        
        with sent_col3:
            st.error("""
            **Negative**:
            - Angry
            - Sad
            - Fearful
            - Disgust
            """)
        
        st.markdown("---")
        
        # Results
        st.subheader("Outstanding Performance")
        
        st.write("""
        The custom Wav2Vec2 sentiment model demonstrates **exceptional accuracy of 96%** 
        across all sentiment classes.
        """)
        
        # Classification Report
        sent_result_col1, sent_result_col2 = st.columns([2, 1])
        
        with sent_result_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_sentiment_classification_report.png"),
                caption="Classification report for Wav2Vec2 sentiment model",
                use_container_width=True
            )
        
        with sent_result_col2:
            st.success("""
            **Performance**:
            
            **Overall Accuracy**: 96%
        
            **Per-Class Metrics**:
            - Consistent high precision
            - Consistent high recall
            - All F1 > 0.95
            
            **Verdict**: Exceptional performance
            """)
        
        st.markdown("---")
        
        # Confusion Matrix
        st.subheader("Confusion Matrix")
        
        sent_conf_col1, sent_conf_col2 = st.columns([2, 1])
        
        with sent_conf_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_sentiment_confusion_matrix.png"),
                caption="Figure 42: Confusion matrix for Wav2Vec2 sentiment model",
                use_container_width=True
            )
        
        with sent_conf_col2:
            st.write("""
            **Classification Reliability**:
            
            **Negative**: 424 correct
            **Neutral**: 400 correct
            **Positive**: 432 correct
            
            **Minimal Misclassification**:
            - Only minor overlap
            - Neutral-Negative confusion < 5%
            - Strong class separation
            """)
        
        st.markdown("---")
        
        # Summary
        st.subheader("Summary")
        
        st.success("""
        **Exceptional Results**:
        
        **96% overall accuracy** - Outstanding sentiment detection  
        **Frozen encoder approach** - Fast adaptation + low overfitting  
        **Lightweight classifier** - Minimal parameters (~10K)  
        **Strong generalization** - Consistent across all sentiments  
        **Production-ready** - High precision and recall for real-world deployment  
        
        **Why This Works**:
        - Freezing encoder prevents overfitting to small labeled data
        - Pre-trained representations capture sentiment cues effectively
        - Sentiment (3-class) simpler than emotion (8-class)
        - Lightweight head easier to optimize
        """)
    
    # ==================== SUBTAB 3: HIERARCHICAL NEGATIVE-EMOTION ====================
    with wav2vec_subtab3:
        st.subheader("Hierarchical Negative-Emotion Recognition")

        st.write("""
        A **two-stage hierarchical pipeline** combining Wav2Vec2 sentiment classification with 
        spectrogram-based CNN for fine-grained negative emotion recognition. This approach reduces 
        confusion across wide emotional categories and improves discrimination within high-arousal negative states.
        """)

        st.markdown("---")

        # Pipeline Overview
        st.subheader("Pipeline Architecture")

        st.code("""
    Audio Input (16 kHz)
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ STAGE 1: Sentiment Classification           â”‚
    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚ Wav2Vec2-Base (Frozen Encoder)              â”‚
    â”‚ + Lightweight Classification Head           â”‚
    â”‚                                             â”‚
    â”‚ Output: Negative / Neutral / Positive       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        â”œâ”€â†’ Neutral â†’ END (Classification Complete)
        â”œâ”€â†’ Positive â†’ END (Classification Complete)
        â””â”€â†’ Negative (1,643 samples) â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ STAGE 2: Negative-Emotion Recognition   â”‚
            â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
            â”‚ Dual-Band Spectrogram Extraction:       â”‚
            â”‚   â€¢ Low-Pass (<3 kHz): Prosodic cues    â”‚
            â”‚   â€¢ High-Pass (>3 kHz): Vocal tension   â”‚
            â”‚                                         â”‚
            â”‚ ResNet-18 Backbone (2-channel input)    â”‚
            â”‚ + Multi-Head Attention Module           â”‚
            â”‚                                         â”‚
            â”‚ Output: Angry / Sad / Fearful / Disgust â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
            Final Emotion Classification
        """, language="text")

        st.markdown("---")

        # Stage 1
        st.header("Stage 1: Sentiment Prediction (Wav2Vec2)")

        st.write("""
        A custom **three-class sentiment classifier** was developed by attaching a lightweight linear 
        classification head to a **frozen Wav2Vec2-Base encoder**. This stage filters out non-negative 
        samples, reducing downstream computational load.
        """)

        st.markdown("---")

        # Stage 1 Architecture
        st.subheader("Architecture Details")

        st.write("**Wav2Vec2-Base Encoder with Lightweight Head**:")

        stage1_arch_col1, stage1_arch_col2 = st.columns([3, 2])

        with stage1_arch_col1:
            st.code("""
Raw Waveform (16 kHz)
        â†“
    Wav2Vec2-Base Encoder (FROZEN)
        â”œâ”€ Feature Extractor (CNN)
        â”œâ”€ Transformer Encoder (12 layers)
        â””â”€ Hidden States (768-dim)
        â†“
    Temporal Average Pooling
        â†“
    Linear Classification Head
        â”œâ”€ Dense(768 â†’ 256)
        â”œâ”€ ReLU + Dropout(0.3)
        â””â”€ Dense(256 â†’ 3)
        â†“
    Softmax
        â†“
    Output: Sentiment Probabilities
        (Negative / Neutral / Positive)
            """, language="text")

        with stage1_arch_col2:
            st.info("""
            **Design Benefits**:
            
            **Frozen Encoder**:
            - Prevents catastrophic forgetting
            - Fast training
            - Stable learning
            
            **Lightweight Head**:
            - Minimal parameters
            - Reduced overfitting
            - ~10K trainable params
            """)
        
        st.markdown("---")
        
        # Negative Sample Extraction
        st.subheader("Negative Sample Extraction")

        extract_col1, extract_col2 = st.columns([3, 2])

        with extract_col1:
            st.image(
                str(IMG_DIR / "wav2vec2_negative_prediction.png"),
                caption="Prediction for next stage",
                use_container_width=True
            )

        with extract_col2:
            st.write("""
            **Extraction Process**:

            1. **Run Stage 1** on all test samples
            2. **Filter predictions** where class = Negative
            3. **Extract samples**: 1,643 negative predictions
            4. **Save to CSV** with metadata
            5. **Forward to Stage 2** for fine-grained classification

            **Why This Works**:
            - Reduces Stage 2 input by ~67%
            - Focuses on emotionally dense samples
            - Simplifies downstream task
            """)

        st.markdown("---")

        # Stage 2
        st.header("Stage 2: Fine-Grained Negative-Emotion Classification")

        st.write("""
        After obtaining sentiment predictions, all samples predicted as **Negative** (1,643 samples) 
        are isolated for fine-grained classification among four negative emotions: **angry, sad, fearful, disgust**.
        """)

        st.markdown("---")

        # Dual-Band Spectrogram Extraction
        st.subheader("Dual-Band Spectrogram Feature Extraction")

        st.write("""
        Each negative sample is transformed into **two complementary spectrogram channels** 
        to capture both prosodic and acoustic-tension cues.
        """)

        feature_col1, feature_col2, feature_col3 = st.columns(3)

        with feature_col1:
            st.error("""
            **Low-Pass (<3 kHz)**:

            Captures **prosodic cues**:
            - Pitch contours (F0)
            - Energy contour
            - Vowel resonance (formants)
            - Speech intonation patterns
            - Emotional prosody

            **Why?**: Low frequencies carry 
            fundamental emotional information 
            (sad = low pitch, happy = high pitch)
            """)

        with feature_col2:
            st.warning("""
            **High-Pass (>3 kHz)**:

            Isolates **high-frequency features**:
            - Turbulent noise
            - Vocal tension
            - Fricative consonants
            - Anger/fear indicators
            - Breathy voice quality

            **Why?**: High frequencies indicate 
            arousal and tension (anger = harsh, 
            fear = breathy)
            """)

        with feature_col3:
            st.info("""
            **2-Channel Combination**:

            **Input format**:
            - Shape: (128, Time, 2)
            - Channel 1: Low-pass Mel-spec
            - Channel 2: High-pass Mel-spec

            **Benefits**:
            - Complementary information
            - Better emotion discrimination
            - ResNet-18 compatible
            - Preserves frequency details
            """)

        st.markdown("---")

    # CNN Baseline for Negative Emotions
    with st.expander("CNN Baseline Model"):
        st.write("""
        A lightweight CNN baseline was developed 
        to establish initial performance benchmarks for negative emotion classification.
        """)

        st.subheader("Architecture")

        cnn_arch_col1, cnn_arch_col2 = st.columns([2, 1])

        with cnn_arch_col1:
            st.code("""
2-Channel Mel-Spectrogram (Low + High Pass)
    â†“
Conv Block 1 (32 filters, 3Ã—3)
    â”œâ”€ BatchNorm + ReLU
    â””â”€ MaxPool (2Ã—2)
    â†“
Conv Block 2 (64 filters, 3Ã—3)
    â”œâ”€ BatchNorm + ReLU
    â””â”€ MaxPool (2Ã—2)
    â†“
Conv Block 3 (128 filters, 3Ã—3)
    â”œâ”€ BatchNorm + ReLU
    â””â”€ AdaptiveAvgPool (8Ã—T)
    â†“
Frequency Averaging â†’ [B, 128, T']
    â†“
Multi-Head Attention (4 heads)
    â”œâ”€ Self-attention over time
    â””â”€ Global temporal pooling
    â†“
Fully Connected Layers
    â”œâ”€ FC1 (128 â†’ 256) + ReLU
    â”œâ”€ Dropout (0.4)
    â””â”€ FC2 (256 â†’ 4) + Softmax
    â†“
Output: Negative Emotion Probabilities
        """, language="text")

        with cnn_arch_col2:
            st.info("""
            **Model Specs**:
            
            **Parameters**: ~250K
            **Classes**: 4 (angry, sad, fearful, disgust)
            **Input**: 2-channel spectrograms
            **Optimizer**: Adam (lr=1e-4)
            **Epochs**: 100 (early stopping)
            
            **Key Features**:
            - Lightweight architecture
            - Multi-head attention
            - Dual-band input
            """)

        st.markdown("---")

        st.subheader("Training Configuration")

        config_col1, config_col2 = st.columns(2)

        with config_col1:
            st.markdown("""
            **Data**:
            - Training samples: 1,643 negative emotions
            - Input: Dual-band spectrograms (low-pass + high-pass)
            - Batch size: 32
            - Loss: Cross-Entropy

            **Architecture Highlights**:
            - 3 convolutional blocks (32 â†’ 64 â†’ 128 filters)
            - AdaptiveAvgPool preserves temporal resolution
            - Multi-head attention (4 heads) over time dimension
            """)

        with config_col2:
            st.markdown("""
            **Training**:
            - Optimizer: Adam (lr = 1e-4)
            - Epochs: 100 with early stopping
            - Regularization: Dropout (0.4), BatchNorm
            - Best model saved based on validation accuracy

            **Output**:
            - Labels: angry, sad, fearful, disgust
            """)

        st.markdown("---")

        # ========== PERFORMANCE RESULTS ==========
        st.subheader("Performance Results")

        # Metrics Summary
        perf_col1, perf_col2, perf_col3 = st.columns(3)

        with perf_col1:
            st.metric("Test Accuracy", "60%")
            st.caption("4-class negative emotions")

        with perf_col2:
            st.metric("Parameters", "~250K")
            st.caption("Lightweight model")

        with perf_col3:
            st.metric("Training Time", "~15 min")
            st.caption("100 epochs")

        st.markdown("---")

        # Classification Report & Confusion Matrix
        st.subheader("Detailed Performance Analysis")

        result_col1, result_col2 = st.columns([1, 1])

        with result_col1:
            st.write("**Classification Report**:")
            st.image(
                str(IMG_DIR / "cnn_baseline_classification_report.png"),
                caption="Classification metrics for CNN Baseline (4 negative emotions)",
                use_container_width=True
            )
            
            st.write("""
            **Performance Breakdown**:
            - **Angry**: 0.56 precision, 0.90 recall (best recall)
            - **Sad**: 0.67 precision, 0.46 recall (moderate)
            - **Fearful**: 0.61 precision, 0.73 recall (good)
            - **Disgust**: 0.62 precision, 0.23 recall (challenging)
            
            **Overall Accuracy**: 0.60 (60%)
            """)

        with result_col2:
            st.write("**Confusion Matrix**:")
            st.image(
                str(IMG_DIR / "cnn_baseline_confusion_matrix.png"),
                caption="Confusion patterns for CNN Baseline",
                use_container_width=True
            )
            
            st.write("""
            **Key Observations**:
            - **Angry**: 84/93 correct (strong diagonal)
            - **Sad**: Confused with fearful (18/80)
            - **Fearful**: Good separation (57/78)
            - **Disgust**: High confusion â†’ angry (42/78)
            
            **Common Errors**: Disgust â†’ Angry (54%)
            """)

        st.markdown("---")

        # Performance Analysis
        st.subheader("Analysis")

        analysis_col1, analysis_col2 = st.columns(2)

        with analysis_col1:
            st.success("""
            **Strengths**:
            
            **Angry Detection** (0.90 recall):
            - Best performing class
            - Clear high-energy acoustic features
            - Strong temporal patterns captured by attention
            
            **Fearful Recognition** (0.73 recall):
            - Good separation from other emotions
            - Attention module helps with breathy patterns
            
            **Lightweight & Fast**:
            - Only 250K parameters
            - ~15 min training time
            - Efficient inference (~20ms per sample)
            """)

        with analysis_col2:
            st.warning("""
            **Weaknesses**:
            
            **Disgust Classification** (0.23 recall):
            - Worst performing class
            - 54% confused with angry
            - Harsh voice quality overlap
            
            **Sad-Fearful Confusion**:
            - Both low-energy emotions
            - Similar spectral patterns
            - 22.5% misclassification rate
            
            **Limited Depth**:
            - Only 3 conv blocks
            - Shallow feature hierarchy
            - Misses complex patterns
            """)

        st.markdown("---")

        st.subheader("Why We Moved to ResNet-18")

        reason_col1, reason_col2 = st.columns(2)

        with reason_col1:
            st.error("""
            **CNN Baseline Limitations**:

            **Limited Depth**: Only 3 conv blocks
            - Insufficient hierarchical feature learning
            - Shallow receptive field
            - Can't capture multi-scale patterns

            **Vanishing Gradients**: No skip connections
            - Harder to train deeper networks
            - Gradient flow issues beyond 3-4 layers
            - Limits capacity growth

            **Feature Extraction**: Basic conv layers
            - Less expressive than residual blocks
            - Single-scale feature extraction
            - Misses fine-grained distinctions
            
            **Disgust Classification**:
            - Only 23% recall
            - Too much confusion with angry
            - Needs better feature discrimination
            """)

        with reason_col2:
            st.success("""
            **ResNet-18 Improvements**:

            **Residual Connections**: Skip connections
            - Better gradient flow
            - Enables deeper networks (18+ layers)
            - Prevents degradation problem

            **Deeper Architecture**: 4 residual blocks
            - More hierarchical features
            - Better spectral-temporal modeling
            - Multi-scale pattern learning

            **Pre-trained Weights**: ImageNet initialization
            - Transfer learning benefit
            - Faster convergence
            - Better feature representations
            
            **Improved Accuracy**:
            - 60% â†’ 74.16% (+14.16%)
            - Better disgust discrimination
            - Reduced confusion patterns
            """)

        st.markdown("---")

    # ========== RESNET-18 ARCHITECTURE ==========
    with st.expander("ResNet Architecture & Training Details"):
        st.header("ResNet-18 Architecture & Results")
        st.write("""
        These paired dual-band spectrograms are then used to train a dedicated negative-emotion classifier, 
        implemented using a **ResNet-18 backbone adapted for two-channel audio features**. The convolutional 
        layers extract localized spectral patterns from both bands, while a multi-head attention module 
        aggregates temporal dependencies across the full utterance.
        """)
        st.markdown("---")
        # ResNet-18 Architecture Details
        st.subheader("ResNet-18 Model Architecture")
        st.write("**ResNet-18 Backbone with Multi-Head Attention**:")
        arch2_col1, arch2_col2 = st.columns([3, 2])
        with arch2_col1:
            st.code("""
    2-Channel Mel-Spectrogram (Low + High Pass)
        â†“
    Initial Conv2D (64 filters, 7Ã—7, stride 2)
        â†“
    Max Pooling (3Ã—3, stride 2)
        â†“
    ResNet-18 Backbone:
      â”œâ”€ Residual Block 1 (64 filters Ã— 2)
      â”œâ”€ Residual Block 2 (128 filters Ã— 2, stride 2)
      â”œâ”€ Residual Block 3 (256 filters Ã— 2, stride 2)
      â””â”€ Residual Block 4 (512 filters Ã— 2, stride 2)
        â†“
    Multi-Head Attention Module
      â”œâ”€ Query projection (512 â†’ 512)
      â”œâ”€ Key projection (512 â†’ 512)
      â”œâ”€ Value projection (512 â†’ 512)
      â””â”€ Weighted aggregation across time
        â†“
    Global Average Pooling (spatial)
        â†“
    Feed-Forward Classifier:
      â”œâ”€ Dense(512 â†’ 256) + ReLU + Dropout(0.4)
      â”œâ”€ Dense(256 â†’ 128) + ReLU + Dropout(0.3)
      â””â”€ Dense(128 â†’ 4) + Softmax
        â†“
    Output: Negative Emotion Probabilities
        (Angry / Sad / Fearful / Disgust)
                """, language="text")

        with arch2_col2:
            st.info("""
            **Architecture Highlights**:
            **ResNet-18 Backbone**:
            - Residual connections
            - Skip connections prevent vanishing gradients
            - Deep feature extraction (4 blocks)
            **Multi-Head Attention**:
            - Captures temporal dependencies
            - Weights important time segments
            - Aggregates utterance-level features
            **Classifier**:
            - 2-layer feed-forward
            - Dropout regularization (0.4, 0.3)
            - 4-class softmax output
            **Total Parameters**: ~11M
            **Input Shape**: (128, Time, 2)
            """)
        st.markdown("---")
        # Training Configuration
        st.subheader("Training Configuration")
        train_config_col1, train_config_col2 = st.columns(2)
        with train_config_col1:
            st.markdown("""
            **Model Setup**:
            - Backbone: ResNet-18 (adapted for 2-channel audio)
            - Input: 2-channel Mel-spectrograms
            - Output: 4 negative emotions
            - Loss: Categorical Cross-Entropy
            **Data**:
            - Training samples: 1,643 (negative only)
            - Split: 80% train, 10% val, 10% test
            - Augmentation: SpecAugment, time/freq masking
            """)
        with train_config_col2:
            st.markdown("""
            **Optimization**:
            - Optimizer: Adam (lr = 1e-4)
            - Batch size: 32
            - Epochs: 100 (early stopping)
            - Scheduler: ReduceLROnPlateau
            **Regularization**:
            - Dropout: 0.4 (layer 1), 0.3 (layer 2)
            - L2 weight decay: 1e-5
            - Early stopping patience: 10
            """)
        st.markdown("---")
        # ========== RESNET RESULTS ==========
        st.header("ResNet-18 Performance Results")
        st.write("""
        The ResNetAudio model achieved **74.16% accuracy** for fine-grained negative-emotion classification, 
        which is competitive for CNN-based architectures though lower than transformer-based approaches. 
        The model demonstrates robust feature extraction for high-energy emotions like angry, while lower 
        recall for disgust and fear suggests challenges in distinguishing subtle emotional cues.
        """)
        # Classification Report
        result_col1, result_col2 = st.columns([2, 1])
        with result_col1:
            st.image(
                str(IMG_DIR / "resnet_audio_classification_report.png"),
                caption="Classification Report for ResNetAudio Model",
                use_container_width=True
            )
            st.write("""
            **Performance Breakdown**:
            **Strong Emotions**:
            - **Angry**: Best performance
            - High-energy, clear features
            - Robust feature extraction
            **Moderate**:
            - **Sad**: Moderate recall
            - Low arousal, subtle cues
            **Challenging**:
            - **Fearful**: Lower recall
            - **Disgust**: Underrepresented
            - Subtle emotional distinctions
            """)
        with result_col2:
            st.success("""
            **Performance Summary**:
            **Overall Accuracy**: 74.16%
            **Best Performer**: Angry
            - Clear high-energy acoustic features
            - Strong spectral patterns
            **Challenging Classes**: 
            - Disgust: Subtle cues
            - Fear: Subdued expression
            **vs CNN Baseline**:
            - +14.16% improvement (60% â†’ 74.16%)
            - Better feature extraction
            - Residual learning benefit
            """)
        st.markdown("---")
        # Confusion Matrix
        st.subheader("Confusion Matrix Analysis")
        conf_col1, conf_col2 = st.columns([2, 1])
        with conf_col1:
            st.image(
                str(IMG_DIR / "resnet_audio_confusion_matrix.png"),
                caption="Confusion Matrix for ResNetAudio Model",
                use_container_width=True
            )
        with conf_col2:
            st.write("""
            **Common Confusions**:
            **Fearful â†” Angry**:
            - Both high arousal
            - Similar intensity
            - Vocal tension overlap
            **Sad â†” Fearful**:
            - Low energy overlap
            - Pitch similarities
            **Disgust â†” Angry**:
            - Harsh voice quality
            - High-frequency components
            """)
        st.markdown("---")
        # Analysis
        st.subheader("Analysis & Insights")
        analysis_col1, analysis_col2 = st.columns(2)
        with analysis_col1:
            st.success("""
            **What Works Well**:
            **High-Energy Emotions** (Angry):
            - Clear acoustic features
            - Robust feature extraction
            **Dual-Band Spectrograms**:
            - Complementary frequency info
            - Better than single-channel
            **ResNet-18 Backbone**:
            - Effective residual connections
            - Deep feature hierarchy
            **Attention Module**:
            - Temporal dependency modeling
            - Improves over plain CNN
            """)
        with analysis_col2:
            st.warning("""
            **Challenges Identified**:
            **74.16% Accuracy**:
            - Lower than transformers
            - Competitive for CNN
            **Low Recall** (Disgust/Fear):
            - Underrepresented classes
            - Subtle emotional cues
            **Dataset Limitations**:
            - Only 1,643 samples
            - Class imbalance issues
            **Confusion Patterns**:
            - High-arousal overlap
            - Needs better discrimination
            """)
# ==================== TAB 6: CROSS-DATASET GENERALIZATION (LODO) ====================
with tab5:
    st.header("Cross-Dataset Generalization via LODO")
    
    st.write("""
    To assess cross-corpus robustness and model generalization across different datasets, 
    a **Leave-One-Dataset-Out (LODO)** strategy was employed. This approach evaluates how well 
    models trained on one combination of datasets perform on completely unseen datasets.
    """)
    
    st.markdown("---")
    
    # Methodology
    st.subheader("Methodology")
    
    st.write("""
    **LODO Strategy**:
    - Train on three datasets (combined)
    - Test on the remaining unseen dataset
    - Repeat for each dataset (RAVDESS, TESS, SAVEE, CREMA-D)
    - Classifier architecture: Wav2Vec2 fine-tuned emotion model (consistent across all configurations)
    """)
    
    method_col1, method_col2, method_col3, method_col4 = st.columns(4)
    
    with method_col1:
        st.info("""
        **Config 1**:
        Train: TESS, SAVEE, CREMA-D
        Test: RAVDESS
        """)
    
    with method_col2:
        st.info("""
        **Config 2**:
        Train: RAVDESS, SAVEE, CREMA-D
        Test: TESS
        """)
    
    with method_col3:
        st.info("""
        **Config 3**:
        Train: RAVDESS, TESS, CREMA-D
        Test: SAVEE
        """)
    
    with method_col4:
        st.info("""
        **Config 4**:
        Train: RAVDESS, TESS, SAVEE
        Test: CREMA-D
        """)
    
    st.markdown("---")
    
    # Results Overview
    st.subheader("Evaluation Results")
    
    st.write("""
    The following figure presents **accuracy, precision, recall, and F1-score** for each test dataset 
    under the LODO strategy, revealing cross-dataset generalization challenges.
    """)
    
    # Summary Table
    st.write("**Summary Table of LODO Results**:")
    
    result_col1, result_col2 = st.columns([2, 1])
    
    with result_col1:
        st.image(
            str(IMG_DIR / "lodo_summary_table.png"),
            caption="Summary table of LODO result",
            use_container_width=True
        )
    
    with result_col2:
        st.markdown("""
        **Table Contents**:
        - Test Dataset
        - Accuracy
        - Precision
        - Recall
        - F1-Score
        
        Consolidates all metrics for quick comparison across datasets.
        """)
    
    st.markdown("---")
    
    # Performance Breakdown
    st.subheader("Performance Breakdown by Dataset")
    
    perf_col1, perf_col2 = st.columns([3, 2])
    
    with perf_col1:
        st.image(
            str(IMG_DIR / "lodo_cross_dataset_comparison.png"),
            caption="Figure 40: LODO cross-dataset performance comparison",
            use_container_width=True
        )
    
    with perf_col2:
        st.write("""
        **Key Metrics**:
        
        **Best Performers**:
        - Accuracy: RAVDESS & SAVEE (~0.465)
        - F1-Score: SAVEE (0.459)
        
        **Challenging**:
        - Accuracy: TESS (0.388)
        - F1-Score: TESS (0.353)
        
        **Precision Peak**:
        - TESS: 0.571 (strong acoustic bias)
        """)
    
    st.markdown("---")
    
    # Detailed Analysis
    st.subheader("Detailed Performance Analysis")
    
    analysis_col1, analysis_col2 = st.columns(2)
    
    with analysis_col1:
        st.success("""
        **Accuracy Results**:
        
        **Highest**: RAVDESS & SAVEE (â‰ˆ0.465)
        **Lowest**: TESS (0.388)
        
        **Interpretation**: Models trained on 3 datasets 
        generalize better to SAVEE/RAVDESS than to TESS, 
        suggesting domain-specific acoustic patterns.
        """)
    
    with analysis_col2:
        st.warning("""
        **Precision vs Recall**:
        
        **TESS Peak Precision** (0.571)
        - Strong bias toward certain acoustic features
        - Model conservative in predictions
        - Low recall (struggles with coverage)
        
        **Recall Trends**:
        - Mirrors accuracy trends
        - SAVEE & RAVDESS: Best recall
        - TESS: Lowest recall
        
        **Trade-off**: High precision, low recall 
        suggests model overfits to training acoustic patterns.
        """)
    
    st.markdown("---")
    
    # F1-Score Comparison
    st.subheader("F1-Score Analysis (Harmonic Mean)")
    
    f1_col1, f1_col2, f1_col3, f1_col4 = st.columns(4)
    
    with f1_col1:
        st.metric("SAVEE", "0.459", "Best")
    
    with f1_col2:
        st.metric("RAVDESS", "~0.465*", "Best")
    
    with f1_col3:
        st.metric("CREMA-D", "~0.420*", "")
    
    with f1_col4:
        st.metric("TESS", "0.353", "Lowest")
    
    st.caption("*Estimated from accuracy/precision/recall patterns")
    
    st.write("""
    **Insight**: SAVEE achieved the highest F1-score (0.459), indicating the best balance 
    between precision and recall. TESS's low F1-score (0.353) reflects the challenge of 
    generalizing to its unique acoustic characteristics.
    """)
    
    st.markdown("---")
    
    # Root Causes
    st.subheader("Why TESS Performs Worse: Root Causes")
    
    cause_col1, cause_col2 = st.columns(2)
    
    with cause_col1:
        st.error("""
        **Domain Mismatch Factors**:
        
        **Language**:
        - TESS: English (Toronto Emotional Speech Set)
        - Training data: Mixed languages (RAVDESS, SAVEE, CREMA-D)
        - Phonetic differences affect acoustic patterns
        
        **Speaker Demographics**:
        - TESS: Female speakers only
        - Training: Mixed gender
        - Gender-specific prosody patterns
        
        **Recording Conditions**:
        - TESS: Controlled studio environment
        - Training sets: Varied recording quality
        - Acoustic environment differences
        """)
    
    with cause_col2:
        st.warning("""
        **Acoustic Feature Mismatch**:
        
        **Spectral Characteristics**:
        - TESS frequency distribution differs from training sets
        - Mel-spectrogram patterns speaker-dependent
        - F0 (fundamental frequency) ranges vary
        
        **Temporal Patterns**:
        - Speech rate differences
        - Pause structures vary
        - Emotion expression style differences
        
        **Model Bias**:
        - Trained majority patterns dominate
        - Minority patterns (TESS-specific) underrepresented
        - TESS deviation causes generalization failure
        """)
    
    st.markdown("---")
    
    # Summary Table (Markdown)
    st.subheader("LODO Results Summary Table")
    
    st.markdown("""
    | Test Dataset | Accuracy | Precision | Recall | F1-Score |
    |--------------|----------|-----------|--------|----------|
    | **RAVDESS** | 0.465 | ~0.50 | ~0.465 | ~0.48 |
    | **TESS** | 0.388 | 0.571 | ~0.30 | 0.353 |
    | **SAVEE** | 0.465 | ~0.48 | 0.459 | 0.459 |
    | **CREMA-D** | ~0.420 | ~0.45 | ~0.42 | ~0.42 |
    
    **Average Across Datasets**: ~0.43 accuracy
    
    **Best Generalization**: SAVEE  
    **Worst Generalization**: TESS (-0.077 from best)
    """)
    
    st.markdown("---")
    
    # Implications
    st.subheader("Implications & Insights")
    
    impl_col1, impl_col2 = st.columns(2)
    
    with impl_col1:
        st.success("""
        **What Works Well**:
        
        **SAVEE/RAVDESS Generalization**: 
        - Models transfer well to these datasets
        - Shared acoustic characteristics
        - Similar speaker/recording profiles
        
        **Precision on TESS**:
        - When model predicts, it's often correct (0.571)
        - Indicates learned patterns are valid
        - Conservative approach reduces false positives
        """)
    
    with impl_col2:
        st.warning("""
        **Challenges Identified**:
        
        **Domain Mismatch**:
        - Dataset diversity insufficient for universal model
        - TESS-specific characteristics not well represented
        - Language and gender bias apparent
        
        **Recall Issues on TESS**:
        - Model misses many true emotions (~70% missed)
        - Conservative predictions hurt coverage
        - Needs better handling of TESS acoustic patterns
        """)

    st.markdown("---")
    
    # Recommendations
    st.subheader("Recommendations for Improvement")
    
    rec_col1, rec_col2, rec_col3 = st.columns(3)
    
    with rec_col1:
        st.info("""
        **Data Strategy**:
        
        **Data Augmentation**:
        - SpecAugment for spectral variation
        - Time stretching/pitch shifting
        - Noise injection (background)
        
        **Dataset Balancing**:
        - Oversample TESS samples
        - Domain-aware sampling
        - Language-specific handling
        """)
    
    with rec_col2:
        st.info("""
        **Model Strategy**:
        
        **Transfer Learning**:
        - Pre-train on large audio corpus
        - Fine-tune per dataset
        - Domain adaptation techniques
        
        **Ensemble Methods**:
        - Dataset-specific models
        - Confidence-based weighting
        - Meta-learning approaches
        """)
    
    with rec_col3:
        st.info("""
        **Evaluation Strategy**:
        
        **Robust Validation**:
        - Multi-fold cross-validation
        - Stratified sampling per dataset
        - Speaker-aware splits
        
        **Metric Focus**:
        - Prioritize F1-score
        - Monitor per-dataset performance
        - Track generalization gap
        """)
    
    st.markdown("---")
    
    # Expandable Deep Dive
    with st.expander("Technical Deep Dive: Understanding LODO Results"):
        st.write("""
        ### Statistical Analysis of Performance Gaps
        
        **Accuracy Degradation**:
        - RAVDESS â†’ TESS: -0.077 (17% relative drop)
        - SAVEE â†’ TESS: -0.077 (same magnitude)
        - CREMA-D â†’ TESS: -0.032 (best for TESS)
        
        **Interpretation**:
        CREMA-D shares more acoustic characteristics with TESS, 
        explaining why models trained with CREMA-D generalize slightly better to TESS.
        
        ### Precision-Recall Trade-off
        
        TESS shows inverted P-R relationship:
        - High precision (0.571) â†’ Model selective
        - Low recall (struggles with coverage)
        
        **Why?**:
        Model learns discriminative features from training sets but 
        lacks exposure to TESS-specific acoustic patterns. When it predicts, 
        predictions are usually correct, but it's too conservative.
        
        ### Recommendations by Use Case
        
        **If Accuracy is Priority**:
        - Use SAVEE/RAVDESS production data
        - Avoid TESS without specialized preprocessing
        - Implement domain detection and routing
        
        **If Robustness is Priority**:
        - Invest in data augmentation
        - Use ensemble of dataset-specific models
        - Implement confidence thresholds
        
        **If Universal Model Needed**:
        - Collect more diverse data (more female speakers, languages)
        - Use domain-adversarial training
        - Implement test-time adaptation
        """)
    
    st.markdown("---")
    
    # Conclusion
    st.success("""
    ## Conclusion
    
    The LODO analysis reveals **significant cross-dataset generalization challenges**:
    
    **Key Findings**:
    - **SAVEE/RAVDESS**: Best generalization (~0.465 accuracy)
    - **TESS**: Poorest generalization (0.388 accuracy)
    - **Root Cause**: Domain mismatch (language, gender, recording conditions)
    - **Average Performance**: ~0.43 across all test datasets
    
    **Implications**:
    - Single universal emotion recognition model has **limited real-world applicability**
    - Dataset-specific fine-tuning or domain adaptation **essential** for deployment
    - Wav2Vec2 captures domain-specific patterns that don't generalize well
    
    **Recommendation**:
    For production systems, implement:
    1. **Dataset-specific models** for known sources
    2. **Domain detection** with appropriate model selection
    3. **Confidence thresholds** for uncertain predictions
    4. **Continuous learning** to adapt to new acoustic patterns
    """)